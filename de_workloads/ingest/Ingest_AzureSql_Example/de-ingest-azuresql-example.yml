# Build and deploy pipeline for job Ingest_AzureSql_Example

name: "$(Build.SourceBranchName)-init"

pr:
  branches:
    include:
    - 'main'
  paths:
    include:
      - de_workloads/ingest/Ingest_AzureSql_Example/*

trigger:
  branches:
    include:
      - 'main'
  paths:
    include:
      - de_workloads/ingest/Ingest_AzureSql_Example/*


variables:
  - template: ../../../de_build/job-pipeline-vars.yml
  - template: ../../../de_build/air-data-testing-vars.yml
  - template: ../../../build/version-data-vars.yml
  - name: job  # update job name
    value: 'Ingest_AzureSql_Example'
  - name: jobtype  # update job type
    value: 'ingest'
  - name: include_data_quality  # update data quality option
    value: 'true'
  - name: integration_runtime_name  # TODO: get this from pipeline variables
    value: 'adf-managed-vnet-runtime'
  - name: self_repo_job_dir
    value: '$(self_repo_dir)/de_workloads/$(jobtype)/$(job)'
  - name: self_repo_adf_dir
    value: '$(self_repo_job_dir)/data_factory'
  - name: test_unit_path
    value: '$(self_repo_job_dir)/tests/unit/'
  - name: test_end_to_end_path
    value: '$(self_repo_job_dir)/tests/end_to_end/features/'
  - name: junit_path
    value: '$(self_repo_job_dir)/junit/'
  - name: self_repo_sparkjob_dir
    value: '$(self_repo_job_dir)/spark_jobs'
  - name: self_repo_adf_src
    value: "de_workloads/$(jobtype)/$(job)/data_factory"
  - name: self_repo_blob_config
    value: '$(self_repo_job_dir)/config'
  - name: blob_config_destination
    value: 'config/$(jobtype)/$(job)'
  - name: tf_state_key_dev
    value: $(domain)_$(jobtype)_$(job)_dev
  - name: tf_state_key_prod
    value: $(domain)_$(jobtype)_$(job)_prod

pool:
  name: $(agentpool_name)

stages:
- stage: Build_Stage
  variables:
      - group: amido-stacks-de-pipeline-nonprod
      - name: version_number
        value: "$(version_major).$(version_minor).$(version_revision)"
  jobs:
  - job: Build_Job
    displayName: 'Build_Job'
    steps:
      - task: Bash@3
        displayName: 'Clean Workspace'
        inputs:
          targetType: 'inline'
          script: |
                   echo "Cleaning workspace..."
                   sudo rm -rf $(Build.SourcesDirectory)/*
      - template: ../../../build/azDevOps/templates/air-infrastructure-data-setup.yml
        parameters:
          TaskctlVersion: ${{ variables.TaskctlVersion }}
      - script: |
          lastTag=$(git tag --sort=-creatordate | head -n 1)
          if [[ -z $lastTag ]]; then
            major=$(version_major)
            minor=$(version_minor)
            revision=$(version_revision)
            echo "Last Tag: NOT Present"
          else
            IFS='.' read -ra versionParts <<< "${lastTag#v}"
            major="${versionParts[0]}"
            minor="${versionParts[1]}"
            lastrevision="${versionParts[2]}"
            revision=$((lastrevision + 1))
            echo "Last Tag: $lastTag"
          fi
          newVersion="${major}.${minor}.${revision}"
          echo "New Version: $newVersion"
          echo "##vso[task.setvariable variable=major]$major"
          echo "##vso[task.setvariable variable=minor]$minor"
          echo "##vso[task.setvariable variable=revision]$revision"
          echo "##vso[task.setvariable variable=newVersion]$newVersion"
        displayName: Determine New Version
      - task: Bash@3
        displayName: "TaskCTL: Setup"
        inputs:
         targetType: inline
         script: taskctl setup
        env:
         DOCKER_IMAGE_TAG: $(newVersion)
      - task: Bash@3
        displayName: "TaskCTL: Lint"
        inputs:
          targetType: inline
          script: taskctl lint
        env:
         # Dotnet Build
         CLOUD_PROVIDER: "$(cloud_provider)"
         ARM_TENANT_ID: "$(azure-tenant-id)"
         ARM_SUBSCRIPTION_ID: "$(azure-subscription-id)"
         ARM_CLIENT_ID: "$(azure-client-id)"
         ARM_CLIENT_SECRET: "$(azure-client-secret)"
         TF_FILE_LOCATION: ./$(jobtype)/jobs/$(job)/

#############################################################
# Deploy to non Prod
#############################################################
- stage: Deploy_NonPROD_Stage
  variables:
      - group: amido-stacks-de-pipeline-nonprod
      - group: amido-stacks-infra-credentials-nonprod
      - group: stacks-credentials-nonprod-kv
      - name: version_number
        value: "$(version_major).$(version_minor).$(version_revision)"
  dependsOn: Build_Stage
  jobs:
  - deployment: Deploy_NonPROD
    displayName: 'Deploy To NonPROD'
    environment: ${{ variables.domain }}-nonprod
    pool:
      name: $(agentpool_name)

    strategy:
      runOnce:
        deploy:
          steps:
           - template: ../../../build/azDevOps/templates/air-infrastructure-data-setup.yml
             parameters:
              TaskctlVersion: ${{ variables.TaskctlVersion }}

          # Publish Config files
           - task: AzureCLI@2
             inputs:
          # TODO: Remove hardcoded reference to subscription
               azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
               scriptType: 'pscore'
               scriptLocation: 'inlineScript'
               inlineScript: 'az storage blob upload-batch `
                              --source $(self_repo_blob_config) `
                              --destination $(blob_config_destination) `
                              --account-name $(blob_configStorage) `
                              --overwrite'

           - task: AzureKeyVault@2
             inputs:
                  azureSubscription: 'Stacks.Pipeline.Builds'
                  KeyVaultName: $(keyvault_name)
                  SecretsFilter: '*'
                  RunAsPreJob: false
             displayName: 'Get secrets from the keyvault'

          # TODO: Make the Databricks / spark steps conditional if data quality is not required
          # TODO: Re-add these steps once databricks issue is resolved
          #  - script: |
          #             echo "$(databricks-host)
          #             $(databricks-token)" | databricks configure --token
          #    displayName: Configure databricks-cli

          #  - script: |
          #             folder_exists=$(databricks fs ls dbfs:/FileStore/scripts 2>&1)
          #             echo "folder_exists: $folder_exists"
          #             if [[ $folder_exists == *"No file or directory exists"* ]];
          #             then
          #                 echo "Creating Folder"
          #                 databricks fs mkdirs dbfs:/FileStore/scripts
          #             else
          #                 echo "Folder Exists"
          #             fi
          #    displayName: 'Create Scripts Folder If Not Exists'

          #  - script: |
          #            echo Destination is /FileStore/scripts/
          #            echo Source is $(self_repo_sparkjob_dir)
          #            for file in $(self_repo_sparkjob_dir)/*; do
          #               if [[ -f $file ]]; then
          #                 destination="dbfs:/FileStore/scripts/$(basename $file)"
          #                 databricks fs cp --overwrite "$file" "$destination"
          #               fi
          #            done
          #    displayName: Upload Spark Jobs

    # Publish ADF
           - task: Bash@3
             displayName: "TaskCTL: infrastructure"
             inputs:
              targetType: inline
              script: taskctl -d infrastructure
             env:
              # Dotnet Build
              CLOUD_PROVIDER: "$(cloud_provider)"
              ARM_TENANT_ID: "$(azure-tenant-id)"
              ARM_SUBSCRIPTION_ID: "$(azure-subscription-id)"
              ARM_CLIENT_ID: "$(azure-client-id)"
              ARM_CLIENT_SECRET: "$(azure-client-secret)"
              TF_FILE_LOCATION: "./$(self_repo_adf_src)/"
              ENV_NAME: $(Environment.ShortName)
                    # Azure Authentication
                    # Terraform Backend Configuration
              TF_STATE_CONTAINER: $(tf_state_container)
              TF_STATE_KEY: $(tf_state_key_dev)
              TF_STATE_RG: $(tf_state_rg)
              TF_STATE_STORAGE: $(tf_state_storage)
              TF_BACKEND_ARGS: "key=$(tf_state_key_dev),storage_account_name=$(TF_STATE_STORAGE),resource_group_name=$(TF_STATE_RG),container_name=$(TF_STATE_CONTAINER),subscription_id=$(azure-subscription-id),tenant_id=$(azure-tenant-id),client_id=$(azure-client-id),client_secret= $(azure-client-secret)"
              TF_VAR_data_factory_resource_group_name: $(resource_group)
              TF_VAR_data_factory: $(datafactoryname)
              TF_VAR_integration_runtime_name: $(integration_runtime_name)
              TF_VAR_azuresql_examplesource_connectionstring: "$(sql_connection)"
              TF_VAR_include_data_quality: "$(include_data_quality)"


  #   # Start Testing
  - deployment: Test_NonPROD
    displayName: 'Testing  NonPROD'
    environment: ${{ variables.domain }}-nonprod
    dependsOn: Deploy_NonPROD
    pool:
      name: $(agentpool_name)
    strategy:
      runOnce:
        deploy:
          steps:
           - task: AzureCLI@2
             inputs:
             # TODO: Remove hardcoded reference to subscription
               azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
               ScriptType: 'bash'
               scriptLocation: 'inlineScript'
               inlineScript: az account show

           - task: UsePythonVersion@0
             inputs:
                versionSpec: '$(pythonVersion)'
                githubToken: '$(github_token)'
                addToPath: true

             displayName: Set Python Version

           - bash: |
              pip install pytest pylint pylint-exit pytest-azurepipelines pytest-cov poetry
             displayName: 'Install Pipeline Tools'
           - bash: |
              poetry install
             displayName: 'Running poetry install'
             workingDirectory:  '$(self_repo_dir)'
           - bash: |
               python -m pytest $(test_unit_path)
             displayName: 'Running py test'
             workingDirectory:  '$(self_repo_dir)'

           - bash: |
               poetry run behave $(test_end_to_end_path)  --junit --junit-directory $(junit_path)
             displayName: 'Running e2e Test'
             workingDirectory:  '$(self_repo_dir)'
             env:
                  AZURE_SUBSCRIPTION_ID: $(azure_subscription_id)
                  AZURE_RESOURCE_GROUP_NAME: $(resource_group)
                  AZURE_DATA_FACTORY_NAME: $(datafactoryname)
                  AZURE_REGION_NAME: $(region)
                  AZURE_STORAGE_ACCOUNT_NAME: $(blob_adls_storage)
                  AZURE_CLIENT_ID: $(azure-client-id)
                  AZURE_CLIENT_SECRET: $(azure-client-secret)
                  AZURE_TENANT_ID: $(azure-tenant-id)

           - task: PublishTestResults@2
             displayName: 'Publish Test Results'
             inputs:
                testResultsFiles: '**/*.xml'
                searchFolder: $(junit_path)
             condition: succeededOrFailed()


#############################################################
# Deploy to Prod
#############################################################
- stage: Deploy_Prod_Stage
  dependsOn:
    - Build_Stage
    - Deploy_NonPROD_Stage
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
  variables:
      - group: amido-stacks-de-pipeline-prod
      - group:  stacks-credentials-prod-kv
      - name: version_number
        value: "$(version_major).$(version_minor).$(version_revision)"
  jobs:
  - deployment: Deploy_PRDO
    displayName: 'Deploy To PROD'
    environment: ${{ variables.domain }}-prod
    pool:
      name: $(agentpool_name)

    strategy:
      runOnce:
        deploy:
          steps:

           - task: Bash@3
             displayName: 'Clean Workspace'
             inputs:
               targetType: 'inline'
               script: |
                       echo "Cleaning workspace..."
                       sudo rm -rf $(Build.SourcesDirectory)/*

           - template: ../../../build/azDevOps/templates/air-infrastructure-data-setup.yml
             parameters:
              TaskctlVersion: ${{ variables.TaskctlVersion }}

          # Publish Config files
           - task: AzureCLI@2
             inputs:
             # TODO: Remove hardcoded reference to subscription
               azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
               scriptType: 'pscore'
               scriptLocation: 'inlineScript'
               inlineScript: 'az storage blob upload-batch `
                              --source $(self_repo_blob_config) `
                              --destination $(blob_config_destination) `
                              --account-name $(blob_configStorage) `
                              --overwrite'

           - task: AzureKeyVault@2
             inputs:
                  azureSubscription: 'Stacks.Pipeline.Builds'
                  KeyVaultName: $(keyvault_name)
                  SecretsFilter: '*'
                  RunAsPreJob: false
             displayName: 'Get secrets from the keyvault'

           - script: |
                      echo "$(databricks-host)
                      $(databricks-token)" | databricks configure --token
             displayName: Configure databricks-cli

           - script: |
                      databricks workspace ls
                      databricks fs ls
             displayName: test databricks-cli

           - script: |
                      folder_exists=$(databricks fs ls dbfs:/FileStore/scripts 2>&1)
                      echo "folder_exists: $folder_exists"
                      if [[ $folder_exists == *"No file or directory exists"* ]];
                      then
                          echo "Creating Folder"
                          databricks fs mkdirs dbfs:/FileStore/scripts
                      else
                          echo "Folder Exists"
                      fi
             displayName: 'Create Scripts Folder If Not Exists'

           - script: |
                     echo Destination is /FileStore/scripts/
                     echo Source is $(self_repo_sparkjob_dir)
                     for file in $(self_repo_sparkjob_dir)/*; do
                        if [[ -f $file ]]; then
                          destination="dbfs:/FileStore/scripts/$(basename $file)"
                          databricks fs cp --overwrite "$file" "$destination"
                        fi
                     done
             displayName: Upload Spark Jobs

    # Publish ADF
           - task: Bash@3
             displayName: "TaskCTL: infrastructure"
             inputs:
              targetType: inline
              script: taskctl infrastructure
             env:
              # Dotnet Build
              CLOUD_PROVIDER: "$(cloud_provider)"
              ARM_TENANT_ID: "$(azure-tenant-id)"
              ARM_SUBSCRIPTION_ID: "$(azure-subscription-id)"
              ARM_CLIENT_ID: "$(azure-client-id)"
              ARM_CLIENT_SECRET: "$(azure-client-secret)"
              TF_FILE_LOCATION: "./$(self_repo_adf_src)/"
              ENV_NAME: $(Environment.ShortName)
                    # Azure Authentication
                    # Terraform Backend Configuration
              TF_STATE_CONTAINER: $(tf_state_container)
              TF_STATE_KEY: $(tf_state_key_prod)
              TF_STATE_RG: $(tf_state_rg)
              TF_STATE_STORAGE: $(tf_state_storage)
              TF_BACKEND_ARGS: "key=$(tf_state_key_prod),storage_account_name=$(TF_STATE_STORAGE),resource_group_name=$(TF_STATE_RG),container_name=$(TF_STATE_CONTAINER),subscription_id=$(azure-subscription-id),tenant_id=$(azure-tenant-id),client_id=$(azure-client-id),client_secret= $(azure-client-secret)"
              TF_VAR_data_factory_resource_group_name: $(resource_group)
              TF_VAR_data_factory: $(datafactoryname)
              TF_VAR_integration_runtime_name: $(integration_runtime_name)
              TF_VAR_azuresql_examplesource_connectionstring: "$(sql_connection)"
              TF_VAR_include_data_quality: "$(include_data_quality)"


- stage: Release
  dependsOn:
      - Build_Stage
      - Deploy_Prod_Stage
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'), eq(variables['create_release'], 'true'))
  variables:
      - group: amido-stacks-infra-credentials-nonprod
      # - name: version_number
      #   value: "$(version_major).$(version_minor).$(version_revision)"
  jobs:
      - job: CreateGitHubRelease
        pool:
          name: $(agentpool_name)
        steps:

          - task: Bash@3
            displayName: 'Clean Workspace'
            inputs:
              targetType: 'inline'
              script: |
                      echo "Cleaning workspace..."
                      sudo rm -rf $(Build.SourcesDirectory)/*
          # Check out the repo so that it can be tagged
          - checkout: self
            persistCredentials: true


          - script: |
              lastTag=$(git tag --sort=-creatordate | head -n 1)
              if [[ -z $lastTag ]]; then
                major=$(version_major)
                minor=$(version_minor)
                revision=$(version_revision)
                echo "Last Tag: NOT Present"
              else
                IFS='.' read -ra versionParts <<< "${lastTag#v}"
                major="${versionParts[0]}"
                minor="${versionParts[1]}"
                lastrevision="${versionParts[2]}"
                revision=$((lastrevision + 1))
                echo "Last Tag: $lastTag"
              fi
              newVersion="${major}.${minor}.${revision}"
              echo "New Version: $newVersion"
              echo "##vso[task.setvariable variable=major]$major"
              echo "##vso[task.setvariable variable=minor]$minor"
              echo "##vso[task.setvariable variable=revision]$revision"
              echo "##vso[task.setvariable variable=newVersion]$newVersion"
            displayName: Determine New Version

          - task: Bash@3
            displayName: Tag Code
            inputs:
              targetType: "inline"
              script: |
                commit=$(Build.SourceVersion)
                tag=$(git tag --contains $commit)
                if [ -z "$tag" ]; then
                  echo "Tag does not exist for the commit"
                  git config user.name "BuildService"
                  git config user.email "builder@${COMPANY}.com"
                  echo "Creating tag v${newVersion}..."
                  git tag -a "v${newVersion}" -m "Release created by Azure DevOps"
                  git push origin "v${newVersion}"
                  echo "##vso[task.setvariable variable=ShouldCreateRelease]True"
                else
                  echo "Tag '$tag' already exists for the commit.Skipping tag creation"
                  echo "##vso[task.setvariable variable=ShouldCreateRelease]false"
                fi
            env:
              COMPANY: $(company)
              newVersion: $(newVersion)

          # #           # Create a GitHub release with these packages
          - task: GitHubRelease@1
            displayName: Create GitHub Release
            inputs:
              gitHubConnection: $(github_release_service_connection)
              repositoryName: $(github_org)/$(self_repo)
              tag: v${newVersion}
              releaseNotesSource: 'inline'
              releaseNotesInline: "$(major).$(minor).$(revision)"
              tagSource: 'gitTag'
              changeLogCompareToRelease: 'lastFullRelease'
              changeLogType: 'commitBased'
            condition: eq(variables['ShouldCreateRelease'], 'true')
