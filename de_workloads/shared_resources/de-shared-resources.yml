# Build and deploy pipeline for data engineering workloads shared resources

name: "$(Build.SourceBranchName)-init"

pr:
  branches:
    include:
      - "main"
  paths:
    include:
      - de_workloads/shared_resources/*

trigger:
  branches:
    include:
      - "main"
  paths:
    include:
      - de_workloads/shared_resources/*

variables:
  - template: ../../de_build/job-pipeline-vars.yml
  - template: ../../build/version-data-vars.yml
  - name: job # update job type
    value: "shared_resources"
  - name: include_databricks_resources # update databricks option
    value: "true"
  - name: self_repo_job_dir
    value: "$(self_repo_dir)/de_workloads/$(job)"
  - name: self_repo_adf_dir
    value: "$(self_repo_job_dir)/data_factory"
  - name: test_unit_src
    value: "de_workloads/$(job)/tests/unit/"
  - name: test_end_to_end_src
    value: "de_workloads/$(job)/tests/end_to_end/features/"
  - name: junit_src
    value: "de_workloads/$(jobtype)/$(job)/junit"
  - name: self_repo_sparkjob_dir
    value: "$(self_repo_job_dir)/spark_jobs"
  - name: self_repo_adf_src
    value: "de_workloads/$(job)/data_factory"
  - name: tf_state_key
    value: $(domain)_$(job)

pool:
  name: $(agentpool_name)

stages:
  - stage: Build_Stage
    variables:
      - group: stacks-credentials-nonprod-kv
    jobs:
      - job: Build_Job
        displayName: "Build_Job"
        steps:
          - task: Bash@3
            displayName: "Clean Workspace"
            inputs:
              targetType: "inline"
              script: |
                echo "Cleaning workspace..."
                sudo rm -rf $(Build.SourcesDirectory)/*
                sudo rm -rf $(Build.SourcesDirectory)/.pytest_cache/*
                sudo rm -f $(Build.SourcesDirectory)/.pytest_cache/.gitignore
          - template: ../../build/azDevOps/templates/air-infrastructure-data-setup.yml
            parameters:
              TaskctlVersion: ${{ variables.TaskctlVersion }}
          - script: |
              lastTag=$(git tag --sort=-creatordate | head -n 1)
              if [[ -z $lastTag ]]; then
                major=$(version_major)
                minor=$(version_minor)
                revision=$(version_revision)
                echo "Last Tag: NOT Present"
              else
                IFS='.' read -ra versionParts <<< "${lastTag#v}"
                major="${versionParts[0]}"
                minor="${versionParts[1]}"
                lastrevision="${versionParts[2]}"
                revision=$((lastrevision + 1))
                echo "Last Tag: $lastTag"
              fi
              newVersion="${major}.${minor}.${revision}"
              echo "New Version: $newVersion"
              echo "##vso[task.setvariable variable=major]$major"
              echo "##vso[task.setvariable variable=minor]$minor"
              echo "##vso[task.setvariable variable=revision]$revision"
              echo "##vso[task.setvariable variable=newVersion]$newVersion"
            displayName: Determine New Version
          - task: Bash@3
            displayName: "TaskCTL: Setup"
            inputs:
              targetType: inline
              script: taskctl setup
            env:
              DOCKER_IMAGE_TAG: $(newVersion)
          - task: Bash@3
            displayName: "TaskCTL: Lint"
            inputs:
              targetType: inline
              script: taskctl lint
            env:
              # Dotnet Build
              CLOUD_PROVIDER: "$(cloud_provider)"
              ARM_TENANT_ID: "$(azure-tenant-id)"
              ARM_SUBSCRIPTION_ID: "$(azure-subscription-id)"
              ARM_CLIENT_ID: "$(azure-client-id)"
              ARM_CLIENT_SECRET: "$(azure-client-secret)"
              TF_FILE_LOCATION: "./$(self_repo_adf_src)/"

  #############################################################
  # Deploy to non Prod
  #############################################################
  - stage: Deploy_NonPROD_Stage
    variables:
      - group: amido-stacks-de-pipeline-nonprod
      - group: stacks-credentials-nonprod-kv
      - name: Environment.ShortName
        value: dev
    dependsOn: Build_Stage
    jobs:
      - deployment: Deploy_NonPROD
        displayName: "Deploy To NonPROD"
        environment: ${{ variables.domain }}-nonprod
        pool:
          name: $(agentpool_name)

        strategy:
          runOnce:
            deploy:
              steps:
                - task: Bash@3
                  displayName: "Clean Workspace"
                  inputs:
                    targetType: "inline"
                    script: |
                      echo "Cleaning workspace..."
                      sudo rm -rf $(Build.SourcesDirectory)/*
                      sudo rm -rf $(Build.SourcesDirectory)/.pytest_cache/*
                      sudo rm -f $(Build.SourcesDirectory)/.pytest_cache/.gitignore

                - template: ../../build/azDevOps/templates/air-infrastructure-data-setup.yml
                  parameters:
                    TaskctlVersion: ${{ variables.TaskctlVersion }}

                - task: AzureKeyVault@2
                  inputs:
                    azureSubscription: $(service_connection)
                    KeyVaultName: $(keyvault_name)
                    SecretsFilter: "*"
                    RunAsPreJob: false
                  displayName: "Get secrets from the keyvault"

                # TODO: Make the Databricks / spark steps conditional if data quality is not required
                - script: |
                    echo "$(databricks-host)
                    $(databricks-token)" | databricks configure --token
                  displayName: Configure databricks-cli

                - script: |
                    folder_exists=$(databricks fs ls dbfs:/FileStore/scripts 2>&1)
                    echo "folder_exists: $folder_exists"
                    if [[ $folder_exists == *"No file or directory exists"* ]];
                    then
                        echo "Creating Folder"
                        databricks fs mkdirs dbfs:/FileStore/scripts
                    else
                        echo "Folder Exists"
                    fi
                  displayName: "Create Scripts Folder If Not Exists"

                - script: |
                    echo Destination is /FileStore/scripts/
                    echo Source is $(self_repo_sparkjob_dir)
                    for file in $(self_repo_sparkjob_dir)/*; do
                       if [[ -f $file ]]; then
                         destination="dbfs:/FileStore/scripts/$(basename $file)"
                         databricks fs cp --overwrite "$file" "$destination"
                       fi
                    done
                  displayName: Upload Spark Jobs

                # Publish ADF
                - task: Bash@3
                  displayName: "TaskCTL: infrastructure"
                  inputs:
                    targetType: inline
                    script: taskctl -d infrastructure
                  env:
                    # Dotnet Build
                    CLOUD_PROVIDER: "$(cloud_provider)"
                    ARM_TENANT_ID: "$(azure-tenant-id)"
                    ARM_SUBSCRIPTION_ID: "$(azure-subscription-id)"
                    ARM_CLIENT_ID: "$(azure-client-id)"
                    ARM_CLIENT_SECRET: "$(azure-client-secret)"
                    TF_FILE_LOCATION: "./$(self_repo_adf_src)/"
                    ENV_NAME:
                      $(Environment.ShortName)
                      # Azure Authentication
                      # Terraform Backend Configuration
                    TF_STATE_CONTAINER: $(tf_state_container)
                    TF_STATE_KEY: $(tf_state_key)
                    TF_STATE_RG: $(tf_state_rg)
                    TF_STATE_STORAGE: $(tf_state_storage)
                    TF_BACKEND_ARGS: "key=$(tf_state_key),storage_account_name=$(TF_STATE_STORAGE),resource_group_name=$(TF_STATE_RG),container_name=$(TF_STATE_CONTAINER),subscription_id=$(azure-subscription-id),tenant_id=$(azure-tenant-id),client_id=$(azure-client-id),client_secret= $(azure-client-secret)"
                    TF_VAR_data_factory_resource_group_name: $(resource_group)
                    TF_VAR_data_factory: $(datafactoryname)
                    TF_VAR_integration_runtime_name: $(integration_runtime_name)
                    TF_VAR_blob_configstore_name: $(blob_configStorage)
                    TF_VAR_blob_configstore_endpoint: $(Blob_ConfigStore_serviceEndpoint)
                    TF_VAR_adls_datalake_name: $(blob_adls_storage)
                    TF_VAR_adls_datalake_url: $(ADLS_DataLake_URL)
                    TF_VAR_databricks_workspace_url: "$(databricksHost)"
                    TF_VAR_databricks_workspace_resource_id: "$(databricksWorkspaceResourceId)"
                    TF_VAR_key_vault_resource_group_name: $(resource_group)
                    TF_VAR_key_vault_name: $(keyvault_name)
                    TF_VAR_include_databricks_resources: "$(include_databricks_resources)"

      #   # Start Testing
      - deployment: Test_NonPROD
        displayName: "Testing  NonPROD"
        environment: ${{ variables.domain }}-nonprod
        dependsOn: Deploy_NonPROD
        pool:
          name: $(agentpool_name)
        strategy:
          runOnce:
            deploy:
              steps:
                - task: Bash@3
                  displayName: "TaskCTL: deworkloadTesting"
                  inputs:
                    targetType: inline
                    script: taskctl deworkloadTesting
                  env:
                    WORKING_DIRECTORY: "./"
                    UNIT_TEST_LOCATION: "./$(test_unit_src)"
                    E2E_TEST_LOCATION: "./$(test_end_to_end_src)"
                    JUNIT_LOCATION: "./$(junit_src)"
                    AZURE_SUBSCRIPTION_ID: $(azure-subscription-id)
                    AZURE_RESOURCE_GROUP_NAME: $(resource_group)
                    AZURE_DATA_FACTORY_NAME: $(datafactoryname)
                    AZURE_REGION_NAME: $(region)
                    AZURE_STORAGE_ACCOUNT_NAME: $(blob_adls_storage)
                    AZURE_CLIENT_ID: $(azure-client-id)
                    AZURE_CLIENT_SECRET: $(azure-client-secret)
                    AZURE_TENANT_ID: $(azure-tenant-id)

                - task: PublishTestResults@2
                  displayName: "Publish Test Results"
                  inputs:
                    testResultsFiles: "**/*.xml"
                    searchFolder: '$(System.DefaultWorkingDirectory)/$(junit_src)'
                  condition: succeededOrFailed()

  #############################################################
  # Deploy to Prod
  #############################################################
  - stage: Deploy_Prod_Stage
    dependsOn:
      - Build_Stage
      - Deploy_NonPROD_Stage
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
    variables:
      - group: amido-stacks-de-pipeline-prod
      - group: stacks-credentials-prod-kv
      - name: Environment.ShortName
        value: prod
    jobs:
      - deployment: Deploy_PRDO
        displayName: "Deploy To PROD"
        environment: ${{ variables.domain }}-prod
        pool:
          name: $(agentpool_name)

        strategy:
          runOnce:
            deploy:
              steps:
                - task: Bash@3
                  displayName: "Clean Workspace"
                  inputs:
                    targetType: "inline"
                    script: |
                      echo "Cleaning workspace..."
                      sudo rm -rf $(Build.SourcesDirectory)/*
                      sudo rm -rf $(Build.SourcesDirectory)/.pytest_cache/*
                      sudo rm -f $(Build.SourcesDirectory)/.pytest_cache/.gitignore

                - template: ../../build/azDevOps/templates/air-infrastructure-data-setup.yml
                  parameters:
                    TaskctlVersion: ${{ variables.TaskctlVersion }}

                - task: AzureKeyVault@2
                  inputs:
                    azureSubscription: $(service_connection)
                    KeyVaultName: $(keyvault_name)
                    SecretsFilter: "*"
                    RunAsPreJob: false
                  displayName: "Get secrets from the keyvault"

                - script: |
                    echo "$(databricks-host)
                    $(databricks-token)" | databricks configure --token
                  displayName: Configure databricks-cli

                - script: |
                    databricks workspace ls
                    databricks fs ls
                  displayName: test databricks-cli

                - script: |
                    folder_exists=$(databricks fs ls dbfs:/FileStore/scripts 2>&1)
                    echo "folder_exists: $folder_exists"
                    if [[ $folder_exists == *"No file or directory exists"* ]];
                    then
                        echo "Creating Folder"
                        databricks fs mkdirs dbfs:/FileStore/scripts
                    else
                        echo "Folder Exists"
                    fi
                  displayName: "Create Scripts Folder If Not Exists"

                - script: |
                    echo Destination is /FileStore/scripts/
                    echo Source is $(self_repo_sparkjob_dir)
                    for file in $(self_repo_sparkjob_dir)/*; do
                       if [[ -f $file ]]; then
                         destination="dbfs:/FileStore/scripts/$(basename $file)"
                         databricks fs cp --overwrite "$file" "$destination"
                       fi
                    done
                  displayName: Upload Spark Jobs

                # Publish ADF
                - task: Bash@3
                  displayName: "TaskCTL: infrastructure"
                  inputs:
                    targetType: inline
                    script: taskctl infrastructure
                  env:
                    # Dotnet Build
                    CLOUD_PROVIDER: "$(cloud_provider)"
                    ARM_TENANT_ID: "$(azure-tenant-id)"
                    ARM_SUBSCRIPTION_ID: "$(azure-subscription-id)"
                    ARM_CLIENT_ID: "$(azure-client-id)"
                    ARM_CLIENT_SECRET: "$(azure-client-secret)"
                    TF_FILE_LOCATION: "./$(self_repo_adf_src)/"
                    ENV_NAME:
                      $(Environment.ShortName)
                      # Azure Authentication
                      # Terraform Backend Configuration
                    TF_STATE_CONTAINER: $(tf_state_container)
                    TF_STATE_KEY: $(tf_state_key)
                    TF_STATE_RG: $(tf_state_rg)
                    TF_STATE_STORAGE: $(tf_state_storage)
                    TF_BACKEND_ARGS: "key=$(tf_state_key),storage_account_name=$(TF_STATE_STORAGE),resource_group_name=$(TF_STATE_RG),container_name=$(TF_STATE_CONTAINER),subscription_id=$(azure-subscription-id),tenant_id=$(azure-tenant-id),client_id=$(azure-client-id),client_secret= $(azure-client-secret)"
                    TF_VAR_data_factory_resource_group_name: $(resource_group)
                    TF_VAR_data_factory: $(datafactoryname)
                    TF_VAR_integration_runtime_name: $(integration_runtime_name)
                    TF_VAR_blob_configstore_name: $(blob_configStorage)
                    TF_VAR_blob_configstore_endpoint: $(Blob_ConfigStore_serviceEndpoint)
                    TF_VAR_adls_datalake_name: $(blob_adls_storage)
                    TF_VAR_adls_datalake_url: $(ADLS_DataLake_URL)
                    TF_VAR_databricks_workspace_url: "$(databricksHost)"
                    TF_VAR_databricks_workspace_resource_id: "$(databricksWorkspaceResourceId)"
                    TF_VAR_key_vault_resource_group_name: $(resource_group)
                    TF_VAR_key_vault_name: $(keyvault_name)
                    TF_VAR_include_databricks_resources: "$(include_databricks_resources)"

  - stage: Release
    dependsOn:
      - Build_Stage
      - Deploy_Prod_Stage
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'), eq(variables['create_release'], 'true'))
    jobs:
      - job: CreateGitHubRelease
        pool:
          name: $(agentpool_name)
        steps:
          - task: Bash@3
            displayName: "Clean Workspace"
            inputs:
              targetType: "inline"
              script: |
                echo "Cleaning workspace..."
                sudo rm -rf $(Build.SourcesDirectory)/*
                sudo rm -rf $(Build.SourcesDirectory)/.pytest_cache/*
                sudo rm -f $(Build.SourcesDirectory)/.pytest_cache/.gitignore
          # Check out the repo so that it can be tagged
          - checkout: self
            persistCredentials: true

          - script: |
              lastTag=$(git tag --sort=-creatordate | head -n 1)
              if [[ -z $lastTag ]]; then
                major=$(version_major)
                minor=$(version_minor)
                revision=$(version_revision)
                echo "Last Tag: NOT Present"
              else
                IFS='.' read -ra versionParts <<< "${lastTag#v}"
                major="${versionParts[0]}"
                minor="${versionParts[1]}"
                lastrevision="${versionParts[2]}"
                revision=$((lastrevision + 1))
                echo "Last Tag: $lastTag"
              fi
              newVersion="${major}.${minor}.${revision}"
              echo "New Version: $newVersion"
              echo "##vso[task.setvariable variable=major]$major"
              echo "##vso[task.setvariable variable=minor]$minor"
              echo "##vso[task.setvariable variable=revision]$revision"
              echo "##vso[task.setvariable variable=newVersion]$newVersion"
            displayName: Determine New Version

          - task: Bash@3
            displayName: Tag Code
            inputs:
              targetType: "inline"
              script: |
                commit=$(Build.SourceVersion)
                tag=$(git tag --contains $commit)
                if [ -z "$tag" ]; then
                  echo "Tag does not exist for the commit"
                  git config user.name "BuildService"
                  git config user.email "builder@${COMPANY}.com"
                  echo "Creating tag v${newVersion}..."
                  git tag -a "v${newVersion}" -m "Release created by Azure DevOps"
                  git push origin "v${newVersion}"
                  echo "##vso[task.setvariable variable=ShouldCreateRelease]True"
                else
                  echo "Tag '$tag' already exists for the commit.Skipping tag creation"
                  echo "##vso[task.setvariable variable=ShouldCreateRelease]false"
                fi
            env:
              COMPANY: $(company)
              newVersion: $(newVersion)

          # #           # Create a GitHub release with these packages
          - task: GitHubRelease@1
            displayName: Create GitHub Release
            inputs:
              gitHubConnection: $(github_release_service_connection)
              repositoryName: $(github_org)/$(self_repo)
              tag: v${newVersion}
              releaseNotesSource: "inline"
              releaseNotesInline: "$(major).$(minor).$(revision)"
              tagSource: "gitTag"
              changeLogCompareToRelease: "lastFullRelease"
              changeLogType: "commitBased"
            condition: eq(variables['ShouldCreateRelease'], 'true')
