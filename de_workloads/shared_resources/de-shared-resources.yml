# Build and deploy pipeline for data engineering workloads shared resources

name: "$(Build.SourceBranchName)-init"

parameters:
  - name: force_deploy
    displayName: Force deploy regardless of branch
    type: boolean
    default: false
  - name: deploy
    displayName: Deploy Environment
    type: boolean
    default: true
  - name: destroy
    displayName: Destroy Environment
    type: boolean
    default: false
  - name: stages
    type: object
    default:
      - stage: NonProd
        dependsOn: Build
        condition: ne(variables['Build.SourceBranch'], 'refs/heads/main')
        environment_shortname: nonprod
      - stage: Prod
        dependsOn: Build
        condition: eq(variables['Build.SourceBranch'], 'refs/heads/main')
        environment_shortname: prod
  - name: debug
    displayName: Turn on debug for the Independent Runner
    type: boolean
    default: false

pr:
  branches:
    include:
      - "main"
  paths:
    include:
      - de_workloads/shared_resources/*

trigger:
  branches:
    include:
      - "main"
  paths:
    include:
      - de_workloads/shared_resources/*

variables:
  - template: ../../build/common-vars.yml
  - template: ../../de_build/job-pipeline-vars.yml
  - template: ../../build/version-data-vars.yml
  - name: job
    value: "shared_resources"
  - name: include_databricks_resources # update databricks option
    value: "true"
  - name: self_repo_job_dir
    value: "$(self_repo_dir)/de_workloads/$(job)"
  - name: self_repo_adf_dir
    value: "$(self_repo_job_dir)/data_factory"
  - name: test_unit_src
    value: "de_workloads/$(job)/tests/unit/"
  - name: test_end_to_end_src
    value: "de_workloads/$(job)/tests/end_to_end/features/"
  - name: junit_src
    value: "de_workloads/$(job)/junit"
  - name: self_repo_sparkjob_dir
    value: "$(self_repo_job_dir)/spark_jobs"
  - name: self_repo_adf_src
    value: "de_workloads/$(job)/data_factory"
  - name: tf_state_key
    value: $(domain)_$(job)

pool:
  name: $(agentpool_name)

stages:
  - stage: Build
    variables:
      - group: ensono-sp-creds
    jobs:
      - job: Build_Job
        displayName: "Build_Job"
        steps:
          - task: Bash@3
            displayName: "Clean Workspace"
            inputs:
              targetType: "inline"
              script: |
                echo "Cleaning workspace..."
                sudo rm -rf $(Build.SourcesDirectory)/*
                sudo rm -rf $(Build.SourcesDirectory)/.pytest_cache/*
                sudo rm -f $(Build.SourcesDirectory)/.pytest_cache/.gitignore
          - template: ../../build/azDevOps/templates/air-infrastructure-data-setup.yml
            parameters:
              TaskctlVersion: ${{ variables.TaskctlVersion }}
          - script: |
              lastTag=$(git tag --sort=-creatordate | head -n 1)
              if [[ -z $lastTag ]]; then
                major=$(version_major)
                minor=$(version_minor)
                revision=$(version_revision)
                echo "Last Tag: NOT Present"
              else
                IFS='.' read -ra versionParts <<< "${lastTag#v}"
                major="${versionParts[0]}"
                minor="${versionParts[1]}"
                lastrevision="${versionParts[2]}"
                revision=$((lastrevision + 1))
                echo "Last Tag: $lastTag"
              fi
              newVersion="${major}.${minor}.${revision}"
              echo "New Version: $newVersion"
              echo "##vso[task.setvariable variable=major]$major"
              echo "##vso[task.setvariable variable=minor]$minor"
              echo "##vso[task.setvariable variable=revision]$revision"
              echo "##vso[task.setvariable variable=newVersion]$newVersion"
            displayName: Determine New Version
          - task: Bash@3
            displayName: "TaskCTL: Setup"
            inputs:
              targetType: inline
              script: taskctl setup
            env:
              DOCKER_IMAGE_TAG: $(newVersion)
          - task: Bash@3
            displayName: "TaskCTL: Lint"
            inputs:
              targetType: inline
              script: taskctl lint
            env:
              # Dotnet Build
              CLOUD_PLATFORM: "$(cloud_platform)"
              TF_FILE_LOCATION: "./$(self_repo_adf_src)/"

  - ${{ each stage in parameters.stage }}:
      - stage: ${{ stage.stage }}
        dependsOn: ${{ stage.dependsOn }}
        condition: and(succeeded(), or(${{ stage.condition }}, ${{ parameters.force_deploy }}))
        variables:
          - group: ensono-sp-creds
          - group: amido-stacks-de-pipeline-${{ stage.environment_shortname }}

        jobs:
          - deployment: Deploy-${{ stage.stage }}
            environment: ${{ variables.domain }}-${{ stage.environment_shortname }}
            pool:
              name: $(agentpool_name)

            strategy:
              runOnce:
                deploy:
                  steps:
                    - task: Bash@3
                      displayName: "Clean Workspace"
                      inputs:
                        targetType: "inline"
                        script: |
                          echo "Cleaning workspace..."
                          sudo rm -rf $(Build.SourcesDirectory)/*
                          sudo rm -rf $(Build.SourcesDirectory)/.pytest_cache/*
                          sudo rm -f $(Build.SourcesDirectory)/.pytest_cache/.gitignore

                    - template: ../../build/azDevOps/templates/air-infrastructure-data-setup.yml
                      parameters:
                        TaskctlVersion: ${{ variables.TaskctlVersion }}

                    - task: AzureKeyVault@2
                      inputs:
                        azureSubscription: $(service_connection)
                        KeyVaultName: $(keyvault_name)
                        SecretsFilter: "*"
                        RunAsPreJob: false
                      displayName: "Get secrets from the keyvault"

                    # TODO: Make the Databricks / spark steps conditional if data quality is not required
                    - script: |
                        echo "$(databricks-host)
                        $(databricks-token)" | databricks configure --token
                      displayName: Configure databricks-cli

                    - script: |
                        folder_exists=$(databricks fs ls dbfs:/FileStore/scripts 2>&1)
                        echo "folder_exists: $folder_exists"
                        if [[ $folder_exists == *"No file or directory exists"* ]];
                        then
                            echo "Creating Folder"
                            databricks fs mkdirs dbfs:/FileStore/scripts
                        else
                            echo "Folder Exists"
                        fi
                      displayName: "Create Scripts Folder If Not Exists"

                    - script: |
                        echo Destination is /FileStore/scripts/
                        echo Source is $(self_repo_sparkjob_dir)
                        for file in $(self_repo_sparkjob_dir)/*; do
                          if [[ -f $file ]]; then
                            destination="dbfs:/FileStore/scripts/$(basename $file)"
                            databricks fs cp --overwrite "$file" "$destination"
                          fi
                        done
                      displayName: Upload Spark Jobs

                    # Publish ADF
                    - task: Bash@3
                      displayName: "TaskCTL: infrastructure"
                      inputs:
                        targetType: inline
                        script: taskctl infrastructure
                      env:
                        # Dotnet Build
                        CLOUD_PLATFORM: "$(cloud_platform)"
                        TF_FILE_LOCATION: "./$(self_repo_adf_src)/"
                        ENV_NAME:
                          $(Environment.ShortName)
                          # Azure Authentication
                          # Terraform Backend Configuration

                        TF_BACKEND_ARGS: "key=$(tf_state_key),storage_account_name=$(tf_state_storage),resource_group_name=$(tf_state_rg),container_name=$(tf_state_container)"
                        TF_VAR_data_factory_resource_group_name: $(resource_group)
                        TF_VAR_data_factory: $(datafactoryname)
                        TF_VAR_integration_runtime_name: $(integration_runtime_name)
                        TF_VAR_blob_configstore_name: $(blob_configStorage)
                        TF_VAR_blob_configstore_endpoint: $(Blob_ConfigStore_serviceEndpoint)
                        TF_VAR_adls_datalake_name: $(blob_adls_storage)
                        TF_VAR_adls_datalake_url: $(ADLS_DataLake_URL)
                        TF_VAR_databricks_workspace_url: "$(databricksHost)"
                        TF_VAR_databricks_workspace_resource_id: "$(databricksWorkspaceResourceId)"
                        TF_VAR_key_vault_resource_group_name: $(resource_group)
                        TF_VAR_key_vault_name: $(keyvault_name)
                        TF_VAR_include_databricks_resources: "$(include_databricks_resources)"

          - deployment: Test-${{ stage.stage }}
            environment: ${{ variables.domain }}-nonprod
            dependsOn: Deploy-${{ stage.stage }}
            pool:
              name: $(agentpool_name)
            strategy:
              runOnce:
                deploy:
                  steps:
                    - template: ../../de_build/de-workload-testing.yml
                      parameters:
                        pythonVersion: $(pythonVersion)
                        githubToken: $(github_token)
                        workingDirectory: "./"
                        unitTestLocation: "./$(test_unit_src)"
                        e2eTestLocation: "./$(test_end_to_end_src)"
                        azureSubscriptionId: $(azure-subscription-id)
                        azureResourceGroupName: $(resource_group)
                        azureDataFactoryName: $(datafactoryname)
                        azureRegionName: $(region)
                        azureStorageAccountName: $(blob_adls_storage)
                        azureConfigAccountName: $(blob_configStorage)
                        azureClientId: $(ARM_CLIENT_ID)
                        azureClientSecret: $(ARM_CLIENT_SECRET)
                        azureTenantId: $(ARM_TENANT_ID)

  - stage: Release
    dependsOn:
      - Build_Stage
      - Deploy_Prod_Stage
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'), eq(variables['create_release'], 'true'))
    jobs:
      - job: CreateGitHubRelease
        pool:
          name: $(agentpool_name)
        steps:
          - task: Bash@3
            displayName: "Clean Workspace"
            inputs:
              targetType: "inline"
              script: |
                echo "Cleaning workspace..."
                sudo rm -rf $(Build.SourcesDirectory)/*
                sudo rm -rf $(Build.SourcesDirectory)/.pytest_cache/*
                sudo rm -f $(Build.SourcesDirectory)/.pytest_cache/.gitignore
          # Check out the repo so that it can be tagged
          - checkout: self
            persistCredentials: true

          - script: |
              lastTag=$(git tag --sort=-creatordate | head -n 1)
              if [[ -z $lastTag ]]; then
                major=$(version_major)
                minor=$(version_minor)
                revision=$(version_revision)
                echo "Last Tag: NOT Present"
              else
                IFS='.' read -ra versionParts <<< "${lastTag#v}"
                major="${versionParts[0]}"
                minor="${versionParts[1]}"
                lastrevision="${versionParts[2]}"
                revision=$((lastrevision + 1))
                echo "Last Tag: $lastTag"
              fi
              newVersion="${major}.${minor}.${revision}"
              echo "New Version: $newVersion"
              echo "##vso[task.setvariable variable=major]$major"
              echo "##vso[task.setvariable variable=minor]$minor"
              echo "##vso[task.setvariable variable=revision]$revision"
              echo "##vso[task.setvariable variable=newVersion]$newVersion"
            displayName: Determine New Version

          - task: Bash@3
            displayName: Tag Code
            inputs:
              targetType: "inline"
              script: |
                commit=$(Build.SourceVersion)
                tag=$(git tag --contains $commit)
                if [ -z "$tag" ]; then
                  echo "Tag does not exist for the commit"
                  git config user.name "BuildService"
                  git config user.email "builder@${COMPANY}.com"
                  echo "Creating tag v${newVersion}..."
                  git tag -a "v${newVersion}" -m "Release created by Azure DevOps"
                  git push origin "v${newVersion}"
                  echo "##vso[task.setvariable variable=ShouldCreateRelease]True"
                else
                  echo "Tag '$tag' already exists for the commit.Skipping tag creation"
                  echo "##vso[task.setvariable variable=ShouldCreateRelease]false"
                fi
            env:
              COMPANY: $(company)
              newVersion: $(newVersion)

          # #           # Create a GitHub release with these packages
          - task: GitHubRelease@1
            displayName: Create GitHub Release
            inputs:
              gitHubConnection: $(github_release_service_connection)
              repositoryName: "$(Build.Repository.Name)"
              tag: v${newVersion}
              releaseNotesSource: "inline"
              releaseNotesInline: "$(major).$(minor).$(revision)"
              tagSource: "gitTag"
              changeLogCompareToRelease: "lastFullRelease"
              changeLogType: "commitBased"
            condition: eq(variables['ShouldCreateRelease'], 'true')
