tasks:

  buildnumber:
    context: powershell
    description: Update the build number
    command:
      - Update-BuildNumber

  lint:yaml:
    context: powershell-python
    description: Perform YAML linting
    command:
      - Invoke-YamlLint

  lint:terraform:format:
    context: powershell
    description: Perform Terraform format check
    command:
      - Invoke-Terraform -Format -Path $env:TF_FILE_LOCATION

  lint:terraform:validate:
    context: powershell
    description: Perform Terraform validation
    command:
      - Invoke-Terraform -Validate -Path $env:TF_FILE_LOCATION

  # Infrastructure tasks
  infra:init:
    context: powershell
    description: Initialise Terraform for Azure
    command:
      - Invoke-Terraform -Init -backend "$env:TF_BACKEND_ARGS" -Path $env:TF_FILE_LOCATION -Debug
      - Invoke-Terraform -Workspace -Arguments $env:ENV_NAME -Path $env:TF_FILE_LOCATION -Debug

  infra:plan:
    context: powershell
    description: Plan Terraform
    command:
      - Invoke-Terraform -Plan -Path $env:TF_FILE_LOCATION -Arguments "-input=false","-out=tfplan" -Debug | out-null
  infra:apply:
    context: powershell
    description: Apply Terraform Plan
    command:
    #  - Push-Location $env:TF_FILE_LOCATION && Invoke-Terraform -Apply -Path tfplan -Debug | out-null
      - Push-Location $env:TF_FILE_LOCATION && Invoke-Terraform -Apply -Path tfplan -Debug

  publish:github:
    context: powershell
    description: Publish Release to GitHub
    command:
      - Publish-GitHubRelease -artifactsList "$env:ARTIFACTS_LIST"
    env:
      generateReleaseNotes: $true
      # PUBLISH_RELEASE: $true

  update:dashboard:
    context: powershell
    description: Update the Deployment Dashboard
    command:
      - Update-InfluxDashboard
    # yamllint disable rule:comments-indentation
    # env:
    # PUBLISH_RELEASE: $true
    # yamllint enable rule:comments-indentation

  debug:env:
    context: powershell
    description: Debugging task to show the environment variables in the container
    command:
      - dir env:/

  debug:location:
    context: powershell
    command:
      - Write-Host "***** DEBUG *****" && get-childitem -filter "*opencover.xml" -recurse

  debug:sleep:
    context: powershell
    command:
      - echo "Sleeping for {{ .sleep }}"
      - sleep {{ .sleep }}
    variables:
      sleep: 30

# Databricks tasks
  databricks:connect:
    context: powershell
    description: Configure databricks-cli
    command:
       - |
          Write-Output "`nConfigure databricks-cli"
          $args = @"
          $env:DATABRICKS_HOST
          $env:DATABRICKS_TOKEN
          "@
          Write-Output $args | & databricks configure --token 
          Write-Output "`nDebug:Databricks workspace list:"
          & databricks workspace list

  databricks:createdir:
    context: powershell
    description: Create DBFS Dir If Not Exists
    command:
       - |
          Write-Output "
          Create DBFS Dir If Not Exists"
          $destinationPath=$env:DATABRICKS_DBFS_LOCATION
          Write-Output "Required DBFS destination is $destinationPath"
          $folderExists = $(databricks fs ls $destinationPath 2>&1)
          if ($folderExists -like "*No file or directory exists*") {
              Write-Output "Required DBFS directory doesnot Exists.Creating Now"
              # $(databricks fs mkdirs $destinationPath)
          }
          else {
              Write-Output "Required DBFS directory Exists"
          }

  databricks:upload:
    context: powershell
    description: Upload Spark Jobs or Whl to DBFS
    command:
      # - Push-Location $env:TF_FILE_LOCATION && Invoke-Terraform -Apply -Path tfplan -Debug 
        - |    
           Write-Output "
           Upload Spark Jobs or Whl to DBFS"
           $destinationPath=$env:DATABRICKS_DBFS_LOCATION
           $self_repo_sparkjob_dir=$env:DATABRICKS_SOURCE_LOCATION        
           Write-Output "DBFS destination is $destinationPath"
           Write-Output "DBFS Source File location is $self_repo_sparkjob_dir"

            $files = Get-ChildItem -Path $self_repo_sparkjob_dir
            foreach ($file in $files) {
                if ($file -is [System.IO.FileInfo]) {
                    Write-Output  $file.Name
                    $destination = "$destinationPath/$($file.Name)"
                    & databricks fs cp --overwrite "$($file.FullName)" "$destination"
                }
            }