# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

# Sample YAML file to validate and export an ARM template into a build artifact
# Requires a package.json file located in the target repository

name: "$(Build.SourceBranchName)-init"

# trigger:
# - none
pr:
  branches:
    include:
    - 'main'
  paths:
    include:
      - data_processing/jobs/silver/adfpipeline/*
      - data_processing/jobs/silver/tests/*
      - data_processing/jobs/silver/spark-jobs/*
      - data_processing/jobs/silver/silver.yml

trigger:
  branches:
    include:
      - 'main'
  paths:
    include:
      - data_processing/jobs/silver/adfpipeline/*
      - data_processing/jobs/silver/tests/*
      - data_processing/jobs/silver/spark-jobs/*
      - data_processing/jobs/silver/silver.yml


variables:
  - template: ../../../de_build/job-pipeline-vars.yml
  - template: ../../../de_build/air-data-testing-vars.yml
  - template: ../../../build/version-data-vars.yml
  - name: job  #update job name
    value: 'silver'
  - name: jobtest  #update job name
    value: 'dbfs:/FileStore/scriptsTest2'
  - name: jobtype  #update job name
    value: 'data_processing'
  - name: self_repo_adf_src  
    value: "$(jobtype)/jobs/$(job)/adfpipeline"
  - name: self_repo_adf_dir
    value: "$(self_repo_dir)/$(self_repo_adf_src)/"
  - name: test_unit_path
    value: '$(self_repo_dir)/$(jobtype)/jobs/$(job)/tests/unit/'
  - name: test_end_to_end_path
    value: '$(self_repo_dir)/$(jobtype)/jobs/$(job)/tests/end_to_end/features/'
  - name: junit_path
    value: '$(self_repo_dir)/$(jobtype)/jobs/$(job)/junit/'
  - name: self_repo_sparkjob_dir
    value: "$(self_repo_dir)/$(jobtype)/jobs/$(job)/spark-jobs"
  # - name: overrideParameters
  #   value: "-factoryName $(datafactoryname)
  #    -adlsStorageAccountName $(blob_adls_storage)
  #    -blobStorageAccountName $(blob_configStorage)
  #    -databricksHost $(databricksHost)
  #  -databricksWorkspaceResourceId $(databricksWorkspaceResourceId)"
  - name: tf_state_key_dev
    value: $(domain)_$(job)_dev
  - name: tf_state_key_prod
    value: $(domain)_$(job)_prod
  - name: destinationPath
    value: 'dbfs:/FileStore/scriptsTest2'
  - name: databricks_destination_path
    value: 'dbfs:/FileStore/scriptsTest2'

pool:
  name: $(agentpool_name)


stages:
- stage: Build_Stage
  variables:
      - group: amido-stacks-de-pipeline-nonprod
      
      # - name: version_number
      #   value: "$(version_major).$(version_minor).$(version_revision)"

  jobs:
  - job: Build_Job
    displayName: 'Build_Job'

    steps:
      - task: Bash@3
        displayName: 'Clean Workspace'
        inputs:
          targetType: 'inline'
          script: |
                   echo "Cleaning workspace..."
                   sudo rm -rf $(Build.SourcesDirectory)/*
      - template: ../../../build/azDevOps/templates/air-infrastructure-data-setup.yml
        parameters:
          TaskctlVersion: ${{ variables.TaskctlVersion }}
      - script: |
          lastTag=$(git tag --sort=-creatordate | head -n 1)
          if [[ -z $lastTag ]]; then
            major=$(version_major)
            minor=$(version_minor)
            revision=$(version_revision)
            echo "Last Tag: NOT Present"
          else
            IFS='.' read -ra versionParts <<< "${lastTag#v}"
            major="${versionParts[0]}"
            minor="${versionParts[1]}"
            lastrevision="${versionParts[2]}"
            revision=$((lastrevision + 1))
            echo "Last Tag: $lastTag"
          fi
          newVersion="${major}.${minor}.${revision}"
          echo "New Version: $newVersion"
          echo "##vso[task.setvariable variable=major]$major"
          echo "##vso[task.setvariable variable=minor]$minor"
          echo "##vso[task.setvariable variable=revision]$revision"
          echo "##vso[task.setvariable variable=newVersion]$newVersion"
        displayName: Determine New Version
      - task: Bash@3
        displayName: "TaskCTL: Setup"
        inputs:
         targetType: inline
         script: taskctl setup
        env:
         DOCKER_IMAGE_TAG: $(newVersion)   
      # - task: Bash@3
      #   displayName: "TaskCTL: Lint"
      #   inputs:
      #     targetType: inline
      #     script: taskctl lint
      #   env:
      #    # Dotnet Build
      #    CLOUD_PROVIDER: "$(cloud_provider)"
      #    ARM_TENANT_ID: "$(azure-tenant-id)"
      #    ARM_SUBSCRIPTION_ID: "$(azure-subscription-id)"
      #    ARM_CLIENT_ID: "$(azure-client-id)"
      #    ARM_CLIENT_SECRET: "$(azure-client-secret)"
      #    TF_FILE_LOCATION: ./$(jobtype)/jobs/$(job)/ 
      # - template: ../../../de_build/arm-validate.yml
      #   parameters:
      #     self_repo_adf_dir: '$(self_repo_adf_dir)'


#############################################################
# Deploy to non Prod
#############################################################
- stage: Deploy_NonPROD_Stage
  variables:
      - group: amido-stacks-de-pipeline-nonprod
      - group: amido-stacks-infra-credentials-nonprod
      - group: stacks-credentials-nonprod-kv
      - name: version_number
        value: "$(version_major).$(version_minor).$(version_revision)"
      - name: destinationPath
        value: 'dbfs:/FileStore/scriptsTest2'

  dependsOn: Build_Stage
  jobs:
  - deployment: Deploy_NonPROD
    displayName: 'Deploy To NonPROD'
    environment: ${{ variables.domain }}-nonprod
    pool:
      name: $(agentpool_name)

    strategy:
      runOnce:
        deploy:  
          steps:

           - template: ../../../build/azDevOps/templates/air-infrastructure-data-setup.yml
             parameters:
              TaskctlVersion: ${{ variables.TaskctlVersion }}

           - script: dir
             displayName: List  Workspace
             workingDirectory: '$(System.DefaultWorkingDirectory)'

           - script: dir
             displayName: List ArmTemplates
             workingDirectory: '$(self_repo_adf_dir)'

           - script: dir
             displayName: List SparkJob
             workingDirectory: '$(self_repo_sparkjob_dir)'

           - task: AzureKeyVault@2
             inputs:
                  azureSubscription: 'Stacks.Pipeline.Builds'
                  KeyVaultName: $(keyvault_name)
                  SecretsFilter: '*'
                  RunAsPreJob: false
             displayName: 'Get secrets from the keyvault'             
             
          #  - script: |
          #             echo "$(databricks-host)
          #             $(databricks-token)" | databricks configure --token
          #    displayName: Configure databricks-cli

          #  - pwsh: |
          #      $configData = "$(databricks-host)`n$(databricks-token)"
          #      echo $configData | databricks configure --token
          #      databricks workspace ls
          #    displayName: Configure databricks-cli
           - task: Bash@3
             displayName: "TaskCTL: Databricks"
             inputs:
              targetType: inline
              script: taskctl databricks
             env:
              # Dotnet Build
              DATABRICKS_HOST: "$(databricks-host)"
              DATABRICKS_TOKEN: "$(databricks-token)"
              DATABRICKS_DBFS_DESTINATIONPATH: "$(databricks_destination_path)"
              DATABRICKS_SOURCEPATH: "./$(self_repo_sparkjob_dir)"
              TF_FILE_LOCATION: "./$(self_repo_adf_src)/"
              
           - pwsh: |
               $args = @"
               $(databricks-host)
 
               $(databricks-token)
 
               "@
               Write-Output $args | & databricks configure --token 
               Write-Output "`nDatabricks workspace list:"
               & databricks workspace list
               & databricks fs ls
             displayName: Configure databricks-cli


          #  - script: |
          #             folder_exists=$(databricks fs ls dbfs:/FileStore/scripts 2>&1)
          #             echo "folder_exists: $folder_exists"
          #             if [[ $folder_exists == *"No file or directory exists"* ]]; 
          #             then
          #                 echo "Creating Folder"
          #                 databricks fs mkdirs dbfs:/FileStore/scripts
          #             else
          #                 echo "Folder Exists"
          #             fi
          #    displayName: 'Create Scripts Folder If Not Exists'

           - pwsh: |
               $folderExists = $(databricks fs ls $(destinationPath) 2>&1)
               Write-Output "folder_exists: $folderExists"
               if ($folderExists -like "*No file or directory exists*") {
                   Write-Output "Creating Folder"
                   $(databricks fs mkdirs $(destinationPath))
               }
               else {
                   Write-Output "Folder Exists"
               }
             displayName: 'Create Scripts Folder If Not Exists'

           - pwsh: |
               Write-Output "Destination is $(destinationPath)"
               Write-Output "Source is $(self_repo_sparkjob_dir)"
               $files = Get-ChildItem -Path $(self_repo_sparkjob_dir)
               foreach ($file in $files) {
                   if ($file -is [System.IO.FileInfo]) {
                       $destination = "$(destinationPath)/$($file.Name)"
                       & databricks fs cp --overwrite "$($file.FullName)" "$destination"
                   }
               }
             displayName: Upload Spark Jobs

          #  - script: |
          #            echo Destination is /FileStore/scripts/
          #            echo Source is $(self_repo_sparkjob_dir)
          #            for file in $(self_repo_sparkjob_dir)/*; do
          #               if [[ -f $file ]]; then
          #                 destination="dbfs:/FileStore/scripts/$(basename $file)"
          #                 databricks fs cp --overwrite "$file" "$destination"
          #               fi
          #            done
          #    displayName: Upload Spark Jobs  
 

    # # Publish ADF
    #        - task: AzurePowerShell@5
    #          displayName: 'Stop ADF triggers'
    #          inputs:
    #           azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
    #           ScriptType: 'FilePath'
    #           ScriptPath: '$(self_repo_dir)/de_build/adf/PrePostDeploymentScriptLatest.ps1'
    #           ScriptArguments:  -armTemplate "$(self_repo_adf_dir)/ARMTemplateForFactory.json"
    #                           -ResourceGroupName $(resource_group)
    #                           -DataFactoryName $(datafactoryname)
    #                           -predeployment $true
    #                           -deleteDeployment $false
    #           azurePowerShellVersion: 'LatestVersion'

    #        - task: Bash@3
    #          displayName: "TaskCTL: infrastructure"
    #          inputs:
    #           targetType: inline
    #           script: taskctl infrastructure
    #          env:
    #           # Dotnet Build
    #           CLOUD_PROVIDER: "$(cloud_provider)"
    #           ARM_TENANT_ID: "$(azure-tenant-id)"
    #           ARM_SUBSCRIPTION_ID: "$(azure-subscription-id)"
    #           ARM_CLIENT_ID: "$(azure-client-id)"
    #           ARM_CLIENT_SECRET: "$(azure-client-secret)"
    #           TF_FILE_LOCATION: "./$(self_repo_adf_src)/"   #./$(jobtype)/jobs/$(job)/adfpipeline/   #$(SELF_REPO_ADF_DIR)  ./$(jobtype)/jobs/$(job)/adfpipeline/ ./ArmTemplates/
    #           ENV_NAME: $(Environment.ShortName)
    #                 # Azure Authentication
    #                 # Terraform Backend Configuration
    #           TF_STATE_CONTAINER: $(tf_state_container)
    #           TF_STATE_KEY: $(tf_state_key_dev)
    #           TF_STATE_RG: $(tf_state_rg)
    #           TF_STATE_STORAGE: $(tf_state_storage)
    #           TF_BACKEND_ARGS: "key=$(tf_state_key_dev),storage_account_name=$(TF_STATE_STORAGE),resource_group_name=$(TF_STATE_RG),container_name=$(TF_STATE_CONTAINER),subscription_id=$(azure-subscription-id),tenant_id=$(azure-tenant-id),client_id=$(azure-client-id),client_secret= $(azure-client-secret)"
    #                 # Deployment Specific Configuration
    #           TF_VAR_data_factory_resource_group_name: $(resource_group)
    #           TF_VAR_data_factory: $(datafactoryname)
    #           TF_VAR_adlsStorageAccountName: $(blob_adls_storage)
    #           TF_VAR_blobStorageAccountName: $(blob_configStorage)
    #           TF_VAR_databricksHost: "$(databricksHost)"
    #           TF_VAR_databricksWorkspaceResourceId: "$(databricksWorkspaceResourceId)"

    #        - task: AzurePowerShell@5
    #          displayName: 'Clean resources and start ADF triggers'
    #          inputs:
    #           azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
    #           ScriptType: 'FilePath'
    #           ScriptPath: '$(self_repo_dir)/de_build/adf/PrePostDeploymentScriptLatest.ps1'
    #           ScriptArguments:  -armTemplate "$(self_repo_adf_dir)/ARMTemplateForFactory.json"
    #                           -ResourceGroupName $(resource_group)
    #                           -DataFactoryName $(datafactoryname)
    #                           -predeployment $false
    #                           -deleteDeployment $false
    #           azurePowerShellVersion: 'LatestVersion'


#############################################################
# Deploy to Prod
#############################################################
- stage: Deploy_Prod_Stage
  dependsOn: 
    - Build_Stage
    - Deploy_NonPROD_Stage
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
  variables:
      - group: amido-stacks-de-pipeline-prod
      - group:  stacks-credentials-prod-kv
      # - name: version_number
      #   value: "$(version_major).$(version_minor).$(version_revision)"
  jobs:
  - deployment: Deploy_PRDO
    displayName: 'Deploy To PROD'
    environment: ${{ variables.domain }}-prod
    pool:
      name: $(agentpool_name)

    strategy:
      runOnce:
        deploy:  
          steps:

           - task: Bash@3
             displayName: 'Clean Workspace'
             inputs:
               targetType: 'inline'
               script: |
                       echo "Cleaning workspace..."
                       sudo rm -rf $(Build.SourcesDirectory)/*           

           - template: ../../../build/azDevOps/templates/air-infrastructure-data-setup.yml
             parameters:
              TaskctlVersion: ${{ variables.TaskctlVersion }}

           - script: dir
             displayName: List  Workspace
             workingDirectory: '$(System.DefaultWorkingDirectory)'

           - script: dir
             displayName: List ArmTemplates
             workingDirectory: '$(self_repo_adf_dir)'

           - script: dir
             displayName: List SparkJob
             workingDirectory: '$(self_repo_sparkjob_dir)'

           - task: AzureKeyVault@2
             inputs:
                  azureSubscription: 'Stacks.Pipeline.Builds'
                  KeyVaultName: $(keyvault_name)
                  SecretsFilter: '*'
                  RunAsPreJob: false
             displayName: 'Get secrets from the keyvault'             
             
           - script: |
                      echo "$(databricks-host)
                      $(databricks-token)" | databricks configure --token
             displayName: Configure databricks-cli

           - script: |
                      folder_exists=$(databricks fs ls dbfs:/FileStore/scripts 2>&1)
                      echo "folder_exists: $folder_exists"
                      if [[ $folder_exists == *"No file or directory exists"* ]]; 
                      then
                          echo "Creating Folder"
                          databricks fs mkdirs dbfs:/FileStore/scripts
                      else
                          echo "Folder Exists"
                      fi
             displayName: 'Create Scripts Folder If Not Exists'

           - script: |
                     echo Destination is /FileStore/scripts/
                     echo Source is $(self_repo_sparkjob_dir)
                     for file in $(self_repo_sparkjob_dir)/*; do
                        if [[ -f $file ]]; then
                          destination="dbfs:/FileStore/scripts/$(basename $file)"
                          databricks fs cp --overwrite "$file" "$destination"
                        fi
                     done
             displayName: Upload Spark Jobs  
 

    # Publish ADF
           - task: AzurePowerShell@5
             displayName: 'Stop ADF triggers'
             inputs:
              azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
              ScriptType: 'FilePath'
              ScriptPath: '$(self_repo_dir)/de_build/adf/PrePostDeploymentScriptLatest.ps1'
              ScriptArguments:  -armTemplate "$(self_repo_adf_dir)/ARMTemplateForFactory.json"
                              -ResourceGroupName $(resource_group)
                              -DataFactoryName $(datafactoryname)
                              -predeployment $true
                              -deleteDeployment $false
              azurePowerShellVersion: 'LatestVersion'

           - task: Bash@3
             displayName: "TaskCTL: infrastructure"
             inputs:
              targetType: inline
              script: taskctl infrastructure
             env:
              # Dotnet Build
              CLOUD_PROVIDER: "$(cloud_provider)"
              ARM_TENANT_ID: "$(azure-tenant-id)"
              ARM_SUBSCRIPTION_ID: "$(azure-subscription-id)"
              ARM_CLIENT_ID: "$(azure-client-id)"
              ARM_CLIENT_SECRET: "$(azure-client-secret)"
              TF_FILE_LOCATION: "./$(self_repo_adf_src)/"   #./$(jobtype)/jobs/$(job)/adfpipeline/   #$(SELF_REPO_ADF_DIR)  ./$(jobtype)/jobs/$(job)/adfpipeline/ ./ArmTemplates/
              ENV_NAME: $(Environment.ShortName)
                    # Azure Authentication
                    # Terraform Backend Configuration
              TF_STATE_CONTAINER: $(tf_state_container)
              TF_STATE_KEY: $(tf_state_key_prod)
              TF_STATE_RG: $(tf_state_rg)
              TF_STATE_STORAGE: $(tf_state_storage)
              TF_BACKEND_ARGS: "key=$(tf_state_key_prod),storage_account_name=$(TF_STATE_STORAGE),resource_group_name=$(TF_STATE_RG),container_name=$(TF_STATE_CONTAINER),subscription_id=$(azure-subscription-id),tenant_id=$(azure-tenant-id),client_id=$(azure-client-id),client_secret= $(azure-client-secret)"
                    # Deployment Specific Configuration
              TF_VAR_data_factory_resource_group_name: $(resource_group)
              TF_VAR_data_factory: $(datafactoryname)
              TF_VAR_adlsStorageAccountName: $(blob_adls_storage)
              TF_VAR_blobStorageAccountName: $(blob_configStorage)
              TF_VAR_databricksHost: "$(databricksHost)"
              TF_VAR_databricksWorkspaceResourceId: "$(databricksWorkspaceResourceId)"

           - task: AzurePowerShell@5
             displayName: 'Clean resources and start ADF triggers'
             inputs:
              azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
              ScriptType: 'FilePath'
              ScriptPath: '$(self_repo_dir)/de_build/adf/PrePostDeploymentScriptLatest.ps1'
              ScriptArguments:  -armTemplate "$(self_repo_adf_dir)/ARMTemplateForFactory.json"
                              -ResourceGroupName $(resource_group)
                              -DataFactoryName $(datafactoryname)
                              -predeployment $false
                              -deleteDeployment $false
              azurePowerShellVersion: 'LatestVersion'

           
 
- stage: Release
  dependsOn:
      - Build_Stage
      - Deploy_Prod_Stage
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'), eq(variables['create_release'], 'true'))
  variables:
      - group: amido-stacks-infra-credentials-nonprod
      # - name: version_number
      #   value: "$(version_major).$(version_minor).$(version_revision)"
  jobs:
      - job: CreateGitHubRelease
        pool:
          name: $(agentpool_name)
        steps:

          - task: Bash@3
            displayName: 'Clean Workspace'
            inputs:
              targetType: 'inline'
              script: |
                      echo "Cleaning workspace..."
                      sudo rm -rf $(Build.SourcesDirectory)/*    
          # Check out the repo so that it can be tagged
          - checkout: self
            persistCredentials: true

          - script: |
              lastTag=$(git tag --sort=-creatordate | head -n 1)
              if [[ -z $lastTag ]]; then
                major=$(version_major)
                minor=$(version_minor)
                revision=$(version_revision)
                echo "Last Tag: NOT Present"
              else
                IFS='.' read -ra versionParts <<< "${lastTag#v}"
                major="${versionParts[0]}"
                minor="${versionParts[1]}"
                lastrevision="${versionParts[2]}"
                revision=$((lastrevision + 1))
                echo "Last Tag: $lastTag"
              fi
              newVersion="${major}.${minor}.${revision}"
              echo "New Version: $newVersion"
              echo "##vso[task.setvariable variable=major]$major"
              echo "##vso[task.setvariable variable=minor]$minor"
              echo "##vso[task.setvariable variable=revision]$revision"
              echo "##vso[task.setvariable variable=newVersion]$newVersion"
            displayName: Determine New Version

          # Create a new tag with the incremented version
          # - script: |
          #     git config user.name "BuildService"
          #     git config user.email "builder@${COMPANY}.com"
          #     git tag -a ${newVersion} -m "Release created by Azure DevOps"
          #     git push origin ${newVersion}
          #   displayName: Create New Tag

          - task: Bash@3
            displayName: Tag Code
            inputs:
              targetType: "inline"
              script: |
                commit=$(Build.SourceVersion)
                tag=$(git tag --contains $commit)
                if [ -z "$tag" ]; then
                  echo "Tag does not exist for the commit"
                  git config user.name "BuildService"
                  git config user.email "builder@${COMPANY}.com"
                  echo "Creating tag v${newVersion}..."
                  git tag -a "v${newVersion}" -m "Release created by Azure DevOps"
                  git push origin "v${newVersion}"
                  echo "##vso[task.setvariable variable=ShouldCreateRelease]True"
                else
                  echo "Tag '$tag' already exists for the commit.Skipping tag creation"
                  echo "##vso[task.setvariable variable=ShouldCreateRelease]false"
                fi
            env:
              COMPANY: $(company)
              newVersion: $(newVersion)

          # #           # Create a GitHub release with these packages
          - task: GitHubRelease@1
            displayName: Create GitHub Release
            inputs:
              gitHubConnection: $(github_release_service_connection)
              repositoryName: $(github_org)/$(self_repo)
              tag: v${newVersion}
              releaseNotesSource: 'inline'
              releaseNotesInline: "$(major).$(minor).$(revision)"
              tagSource: 'gitTag'
              changeLogCompareToRelease: 'lastFullRelease'
              changeLogType: 'commitBased'
            condition: eq(variables['ShouldCreateRelease'], 'true')