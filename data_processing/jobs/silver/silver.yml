# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

# Sample YAML file to validate and export an ARM template into a build artifact
# Requires a package.json file located in the target repository

name: "$(Build.SourceBranchName)-init"

# trigger:
# - none
pr:
  - main

trigger:
  branches:
   
    include:
      - 'main'
  paths:
    include:
      - data_processing/jobs/silver/adfpipeline/*
      - data_processing/jobs/silver/tests/*
      - data_processing/jobs/silver/silver.yml


variables:
  - template: ../../../de_build/job-pipeline-vars.yml
  - template: ../../../de_build/air-data-testing-vars.yml
  - name: job  #update job name
    value: 'silver'
  - name: jobtype  #update job name
    value: 'data_processing'
  - name: self_repo_adf_dir
    value: "$(self_repo_dir)/$(jobtype)/jobs/$(job)/adfpipeline"
  - name: test_unit_path
    value: '$(self_repo_dir)/$(jobtype)/jobs/$(job)/tests/unit/'
  - name: test_end_to_end_path
    value: '$(self_repo_dir)/$(jobtype)/jobs/$(job)/tests/end_to_end/features/'
  - name: junit_path
    value: '$(self_repo_dir)/$(jobtype)/jobs/$(job)/junit/'
  - name: self_repo_sparkjob_dir
    value: "$(self_repo_dir)/$(jobtype)/jobs/$(job)/spark-jobs"

pool:
  name: $(agentpool_name)


stages:
- stage: Build_Stage
  variables:
      - group: amido-stacks-de-pipeline-nonprod
      - name: version_number
        value: "$(version_major).$(version_minor).$(version_revision)"
      - name: overrideParameters
        value: |
          -factoryName $(datafactoryname)
          -ADLSstorageAccountName $(blob_adls_storage)
          -databricksHost $(databricksHost)
          -databricksWorkspaceResourceId $(databricksWorkspaceResourceId)
  jobs:
   - template: ../../../de_build/build-adf-spark-job.yml
     parameters:
      self_repo_dir: '$(self_repo_dir)'
      self_repo_adf_dir: '$(self_repo_adf_dir)'
      self_repo_sparkjob_dir: '$(self_repo_sparkjob_dir)'

#############################################################
# Deploy to non Prod
#############################################################
- stage: Deploy_NonPROD_Stage
  variables:
      - group: amido-stacks-de-pipeline-nonprod
      - group: amido-stacks-infra-credentials-nonprod
      - name: version_number
        value: "$(version_major).$(version_minor).$(version_revision)"
      - name: overrideParameters
        value: |
          -factoryName $(datafactoryname)
          -ADLSstorageAccountName $(blob_adls_storage)
          -databricksHost $(databricksHost)
          -databricksWorkspaceResourceId $(databricksWorkspaceResourceId)
  dependsOn: Build_Stage
  jobs:
  - deployment: Deploy_NonPROD
    displayName: 'Deploy To NonPROD'
    environment: ${{ variables.domain }}-nonprod
    pool:
      name: $(agentpool_name)

    strategy:
      runOnce:
        deploy:  
          steps:

           - task: DownloadPipelineArtifact@2
             displayName: Download Build Artifacts
             inputs:
              targetPath: '$(System.DefaultWorkingDirectory)'

           - script: dir
             displayName: List ArmTemplates Artifact in Workspace
             workingDirectory: '$(System.DefaultWorkingDirectory)/ArmTemplates'

           - script: dir
             displayName: List ArmTemplates SparkJob in Workspace
             workingDirectory: '$(System.DefaultWorkingDirectory)/SparkJob'

           - script: |
              echo "Variable value: $(overrideParameters)"
             displayName: 'Print Variable Value'

           - script: |
              echo "Variable value: ${{ variables.overrideParameters }}"
             displayName: 'Print Variable Value'

           - script: echo "Stopping pipeline..."
             displayName: 'Stop Pipeline'
             condition: always() # This ensures that the task always runs
    
    # Cancels the pipeline execution
           - script: echo "##vso[task.complete result=Canceled;]Pipeline execution cancelled."
             displayName: 'Cancel Pipeline'

          #  - script: pip install databricks-cli
          #    displayName: Install Databricks CLI

           - task: AzureKeyVault@2
             inputs:
                  azureSubscription: 'Stacks.Pipeline.Builds'
                  KeyVaultName: $(keyvault_name)
                  SecretsFilter: '*'
                  RunAsPreJob: false
             displayName: 'Get secrets from the keyvault'             
             
           - script: |
                      echo "$(databricks-host)
                      $(databricks-token)" | databricks configure --token
             displayName: Configure databricks-cli

           - script: |
                      databricks workspace ls
                      databricks fs ls
             displayName: test databricks-cli

           - script: |
                     echo Destination is /FileStore/scripts/
                     echo Source is $(System.DefaultWorkingDirectory)/SparkJob
                     databricks fs cp -r --overwrite $(System.DefaultWorkingDirectory)/SparkJob/*.* dbfs:/FileStore/scripts/
             displayName: Upload Spark Job      

    # Publish ADF
           - task: AzurePowerShell@5
             displayName: 'Stop ADF triggers'
             inputs:
              azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
              ScriptType: 'FilePath'
              ScriptPath: '$(System.DefaultWorkingDirectory)/ArmTemplates/PrePostDeploymentScriptLatest.ps1'
              ScriptArguments:  -armTemplate "$(System.DefaultWorkingDirectory)/ArmTemplates/ARMTemplateForFactory.json"
                              -ResourceGroupName $(resource_group)
                              -DataFactoryName $(datafactoryname)
                              -predeployment $true
                              -deleteDeployment $false
              azurePowerShellVersion: 'LatestVersion'

           - task: AzureResourceManagerTemplateDeployment@3
             displayName: 'ARM Template deployment: Resource Group scope'
             inputs:
                azureResourceManagerConnection: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
                subscriptionId: '$(azure_subscription_id)'
                resourceGroupName: $(resource_group)
                location: $(region)
                csmFile: '$(System.DefaultWorkingDirectory)/ArmTemplates/ARMTemplateForFactory.json'
                csmParametersFile: '$(System.DefaultWorkingDirectory)/ArmTemplates/ARMTemplateParametersForFactory.json'
                overrideParameters: ${{ variables.overrideParameters }}                             
                deploymentMode: 'Incremental'

           - task: AzurePowerShell@5
             displayName: 'Clean resources and start ADF triggers'
             inputs:
              azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
              ScriptType: 'FilePath'
              ScriptPath: '$(System.DefaultWorkingDirectory)/ArmTemplates/PrePostDeploymentScriptLatest.ps1'
              ScriptArguments:  -armTemplate "$(System.DefaultWorkingDirectory)/ArmTemplates/ARMTemplateForFactory.json"
                              -ResourceGroupName $(resource_group)
                              -DataFactoryName $(datafactoryname)
                              -predeployment $false
                              -deleteDeployment $false
              azurePowerShellVersion: 'LatestVersion'

  #   # Start Testing 
  # - deployment: Test_NonPROD
  #   displayName: 'Testing  NonPROD'
  #   environment: ${{ variables.domain }}-nonprod
  #   dependsOn: Deploy_NonPROD
  #   pool:
  #     name: $(agentpool_name)
  #   strategy:
  #     runOnce:
  #       deploy:  
  #         steps:
  #          - checkout: self
  #         #  - task: AzureCLI@2
  #         #    inputs:
  #         #      azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
  #         #      ScriptType: 'bash'
  #         #      scriptLocation: 'inlineScript'
  #         #      inlineScript: az account show 
           
  #          - task: UsePythonVersion@0
  #            inputs:
  #               versionSpec: '$(pythonVersion)'
  #               githubToken: '$(github_token)'
  #               addToPath: true

  #            displayName: Set Python Version

  #          - bash: |
  #             pip install pytest pylint pylint-exit pytest-azurepipelines pytest-cov poetry
  #            displayName: 'Install Pipeline Tools'
  #          - bash: |
  #             poetry install 
  #            displayName: 'Running poetry install'
  #            workingDirectory:  '$(self_repo_dir)'
  #          - bash: |
  #              python -m pytest $(test_unit_path)
  #            displayName: 'Running py test'
  #            workingDirectory:  '$(self_repo_dir)'

  #          - bash: |
  #              poetry run behave $(test_end_to_end_path)  --junit --junit-directory $(junit_path)
  #            displayName: 'Running e2e Test'
  #            workingDirectory:  '$(self_repo_dir)'
  #            env:
  #                 AZURE_SUBSCRIPTION_ID: $(azure_subscription_id)
  #                 AZURE_RESOURCE_GROUP_NAME: $(resource_group)
  #                 AZURE_DATA_FACTORY_NAME: $(datafactoryname)
  #                 AZURE_REGION_NAME: $(region)
  #                 AZURE_STORAGE_ACCOUNT_NAME: $(blob_adls_storage)
  #                 AZURE_CLIENT_ID: $(azure-client-id)
  #                 AZURE_CLIENT_SECRET: $(azure-client-secret)
  #                 AZURE_TENANT_ID: $(azure-tenant-id)   

  #          - task: PublishTestResults@2
  #            displayName: 'Publish Test Results'
  #            inputs:
  #               testResultsFiles: '**/*.xml'
  #               searchFolder: $(junit_path)
  #            condition: succeededOrFailed()
  

#############################################################
# Deploy to Prod
#############################################################
- stage: Deploy_Prod_Stage
  dependsOn: 
    - Build_Stage
    - Deploy_NonPROD_Stage
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
  variables:
      - group: amido-stacks-de-pipeline-prod
      - name: version_number
        value: "$(version_major).$(version_minor).$(version_revision)"
  jobs:
  - deployment: Deploy_PRDO
    displayName: 'Deploy To PROD'
    environment: ${{ variables.domain }}-prod
    pool:
      name: $(agentpool_name)

    strategy:
      runOnce:
        deploy:  
          steps:


           - task: DownloadPipelineArtifact@2
             displayName: Download Build Artifacts
             inputs:
              targetPath: '$(System.DefaultWorkingDirectory)'

           - script: dir
             displayName: List ArmTemplates Artifact in Workspace
             workingDirectory: '$(System.DefaultWorkingDirectory)/ArmTemplates'

           - script: dir
             displayName: List ArmTemplates SparkJob in Workspace
             workingDirectory: '$(System.DefaultWorkingDirectory)/SparkJob'

          #  - script: pip install databricks-cli
          #    displayName: Install Databricks CLI

           - task: AzureKeyVault@2
             inputs:
                  azureSubscription: 'Stacks.Pipeline.Builds'
                  KeyVaultName: $(keyvault_name)
                  SecretsFilter: '*'
                  RunAsPreJob: false
             displayName: 'Get secrets from the keyvault' 

           - script: |
                      echo "$(databricks-host)
                      $(databricks-token)" | databricks configure --token
             displayName: Configure databricks-cli

           - script: |
                      databricks workspace ls
                      databricks fs ls
             displayName: test databricks-cli

           - script: |
                     echo Destination is /FileStore/scripts/
                     echo Source is $(System.DefaultWorkingDirectory)/SparkJob
                     databricks fs cp -r --overwrite $(System.DefaultWorkingDirectory)/SparkJob/*.* dbfs:/FileStore/scripts/
             displayName: Upload Spark Job   

    # Publish ADF
           - task: AzurePowerShell@5
             displayName: 'Stop ADF triggers'
             inputs:
              azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
              ScriptType: 'FilePath'
              ScriptPath: '$(System.DefaultWorkingDirectory)/ArmTemplates/PrePostDeploymentScriptLatest.ps1'
              ScriptArguments:  -armTemplate "$(System.DefaultWorkingDirectory)/ArmTemplates/ARMTemplateForFactory.json"
                              -ResourceGroupName $(resource_group)
                              -DataFactoryName $(datafactoryname)
                              -predeployment $true
                              -deleteDeployment $false
              azurePowerShellVersion: 'LatestVersion'

           - task: AzureResourceManagerTemplateDeployment@3
             displayName: 'ARM Template deployment: Resource Group scope'
             inputs:
                azureResourceManagerConnection: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
                subscriptionId: '$(azure_subscription_id)'
                resourceGroupName: $(resource_group)
                location: $(region)
                csmFile: '$(System.DefaultWorkingDirectory)/ArmTemplates/ARMTemplateForFactory.json'
                csmParametersFile: '$(System.DefaultWorkingDirectory)/ArmTemplates/ARMTemplateParametersForFactory.json'
                overrideParameters: ${{ variables.overrideParameters }}
                deploymentMode: 'Incremental'

           - task: AzurePowerShell@5
             displayName: 'Clean resources and start ADF triggers'
             inputs:
              azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
              ScriptType: 'FilePath'
              ScriptPath: '$(System.DefaultWorkingDirectory)/ArmTemplates/PrePostDeploymentScriptLatest.ps1'
              ScriptArguments:  -armTemplate "$(System.DefaultWorkingDirectory)/ArmTemplates/ARMTemplateForFactory.json"
                              -ResourceGroupName $(resource_group)
                              -DataFactoryName $(datafactoryname)
                              -predeployment $false
                              -deleteDeployment $false
              azurePowerShellVersion: 'LatestVersion'
 
- stage: Release
  dependsOn:
      - Build_Stage
      - Deploy_Prod_Stage
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'), eq(variables['create_release'], 'true'))
  variables:
      - group: amido-stacks-infra-credentials-nonprod
      - name: version_number
        value: "$(version_major).$(version_minor).$(version_revision)"
  jobs:
      - job: CreateGitHubRelease
        pool:
          name: $(agentpool_name)
        steps:
          # Check out the repo so that it can be tagged
          - checkout: self
            persistCredentials: true

          # Create a tag in the code for this release
          - task: Bash@3
            displayName: Tag Code
            inputs:
              targetType: "inline"
              script: |
                git config user.name "BuildService"
                git config user.email "builder@${COMPANY}.com"
                git tag -a v${VERSION_NUMBER} -m "Release created by Azure DevOps"
                git push origin v${VERSION_NUMBER}
            env:
              COMPANY: $(company)

          # Create a GitHub release with these packages
          - task: GitHubRelease@1
            displayName: Create GitHub Release
            inputs:
              gitHubConnection: $(github_release_service_connection)
              repositoryName: $(github_org)/$(self_repo)
              tag: v${VERSION_NUMBER}
              releaseNotesSource: 'inline'
              releaseNotesInline: "$(version_major).$(version_minor).$(version_revision)"
              tagSource: 'gitTag'
              changeLogCompareToRelease: 'lastFullRelease'
              changeLogType: 'commitBased'
