name: "$(Build.SourceBranchName)-init"

trigger:
#- main #collaboration branch
- none

# pr:
#   - main

# trigger:
#   branches:
   
#     include:
#       - 'main'
#   paths:
#     include:
#       - pysparkle/*
#       - pysparkle/pysparkle-pipeline.yml

variables:
  - template: ../de_build/job-pipeline-vars.yml
  - template: ../de_build/air-data-testing-vars.yml
  - name: self_repo_pysparkle_dir
    value: $(self_repo_dir)/pysparkle
  - name: pythonVersion
    value: 3.10.10
  # - name: test_unit_path
  #   value: '$(self_repo_dir)/ingest/jobs/$(job)/tests/unit/'
  # - name: test_end_to_end_path
  #   value: '$(self_repo_dir)/ingest/jobs/$(job)/tests/end_to_end/features/'
  # - name: junit_path
  #   value: '$(self_repo_dir)/ingest/jobs/$(job)/junit/'
pool:
  vmImage: 'ubuntu-latest'
stages:
  - stage: Build_Stage
    variables:
      - group: amido-stacks-de-pipeline-nonprod
      - name: version_number
        value: $(version_major).$(version_minor).$(version_revision)
    jobs:
      - job: Build_Pysparkle
        displayName: Build_Pysparkle
        steps:
          - task: UsePythonVersion@0
            inputs:
              versionSpec: $(pythonVersion)
              githubToken: $(github_token)
              addToPath: true
            displayName: Set Python Version
          - bash: |
              pip install poetry
            displayName: Install Pipeline Tools
          - bash: |
              poetry install 
            displayName: Install dependencies
            workingDirectory: $(self_repo_pysparkle_dir)
          - bash: |
              poetry build
            displayName: Build the project
            workingDirectory: '$(self_repo_pysparkle_dir)/pysparkle'
          - task: CopyFiles@2
            displayName: Copy Whl
            inputs:
              SourceFolder: '$(self_repo_pysparkle_dir)/dist/'
              Contents: '*.whl'
              TargetFolder: '$(Build.ArtifactStagingDirectory)/pysparklewheel'
          - task: PublishBuildArtifacts@1
            displayName: 'Publish Artifact: pysparklewheel'
            inputs:
              ArtifactName: pysparklewheel
              PathtoPublish: '$(Build.ArtifactStagingDirectory)/pysparklewheel'



#############################################################
# Deploy to non Prod
#############################################################
  - stage: Deploy_NonPROD_Stage
    variables:
        - group: amido-stacks-de-pipeline-nonprod
        - group: amido-stacks-infra-credentials-nonprod
        - name: version_number
          value: "$(version_major).$(version_minor).$(version_revision)"
    dependsOn: Build_Stage
    jobs:
    - deployment: Deploy_NonPROD
      displayName: 'Deploy To NonPROD'
      environment: ${{ variables.domain }}-nonprod
      pool:
        name: $(agentpool_name)

      strategy:
        runOnce:
          deploy:  
            steps:

            - task: DownloadPipelineArtifact@2
              displayName: Download Build Artifacts
              inputs:
                targetPath: '$(System.DefaultWorkingDirectory)'

            - script: dir
              displayName: List Artifact in Workspace
              workingDirectory: '$(System.DefaultWorkingDirectory)/pysparklewheel'

            - script: pip install databricks-cli
              displayName: Install Databricks CLI
              
            - script: |
                        echo "$(databricks-token)
                        $(databricks-token)" | databricks configure --token
              displayName: Configure databricks-cli

            - script: |
                        databricks workspace ls
                        databricks fs ls
              displayName: test databricks-cli

            - script: |
                      echo Destination is /FileStore
                      echo Source is $(System.DefaultWorkingDirectory)/Databricks/DBFS-Files
                      databricks fs cp -r --overwrite $(System.DefaultWorkingDirectory)/pysparklewheel/*.whl dbfs:/FileStore/pysparkl_latest.whl

              displayName: Upload whl
              
        

      # Publish ADF


    #   # Start Testing 
    # - deployment: Test_NonPROD
    #   displayName: 'Testing  NonPROD'
    #   environment: ${{ variables.domain }}-nonprod
    #   dependsOn: Deploy_NonPROD
    #   pool:
    #     name: $(agentpool_name)
    #   strategy:
    #     runOnce:
    #       deploy:  
    #         steps:
    #          - checkout: self
    #          - task: AzureCLI@2
    #            inputs:
    #              azureSubscription: 'amido.stacks (719637e5-aedd-4fb1-b231-5101b45f8bb5)'
    #              ScriptType: 'bash'
    #              scriptLocation: 'inlineScript'
    #              inlineScript: az account show 
            
    #          - task: UsePythonVersion@0
    #            inputs:
    #               versionSpec: '$(pythonVersion)'
    #               githubToken: '$(github_token)'
    #               addToPath: true

    #            displayName: Set Python Version

    #          - bash: |
    #             pip install pytest pylint pylint-exit pytest-azurepipelines pytest-cov poetry
    #            displayName: 'Install Pipeline Tools'
    #          - bash: |
    #             poetry install 
    #            displayName: 'Running poetry install'
    #            workingDirectory:  '$(self_repo_dir)'
    #          - bash: |
    #              python -m pytest $(test_unit_path)
    #            displayName: 'Running py test'
    #            workingDirectory:  '$(self_repo_dir)'

    #          - bash: |
    #              poetry run behave $(test_end_to_end_path)  --junit --junit-directory $(junit_path)
    #            displayName: 'Running e2e Test'
    #            workingDirectory:  '$(self_repo_dir)'
    #            env:
    #                 AZURE_SUBSCRIPTION_ID: $(azure_subscription_id)
    #                 AZURE_RESOURCE_GROUP_NAME: $(resource_group)
    #                 AZURE_DATA_FACTORY_NAME: $(datafactoryname)
    #                 AZURE_REGION_NAME: $(region)
    #                 AZURE_STORAGE_ACCOUNT_NAME: $(blob_adls_storage)
    #                 AZURE_CLIENT_ID: $(azure-client-id)
    #                 AZURE_CLIENT_SECRET: $(azure-client-secret)
    #                 AZURE_TENANT_ID: $(azure-tenant-id)   

    #          - task: PublishTestResults@2
    #            displayName: 'Publish Test Results'
    #            inputs:
    #               testResultsFiles: '**/*.xml'
    #               searchFolder: $(junit_path)
    #            condition: succeededOrFailed()
    

  #############################################################
  # Deploy to Prod
  #############################################################
  - stage: Deploy_Prod_Stage
    dependsOn: 
      - Build_Stage
      - Deploy_NonPROD_Stage
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
    variables:
        - group: amido-stacks-de-pipeline-prod
        - name: version_number
          value: "$(version_major).$(version_minor).$(version_revision)"
    jobs:
    - deployment: Deploy_PRDO
      displayName: 'Deploy To PROD'
      environment: ${{ variables.domain }}-prod
      pool:
        name: $(agentpool_name)

      strategy:
        runOnce:
          deploy:  
            steps:


            - task: DownloadPipelineArtifact@2
              displayName: Download Build Artifacts
              inputs:
                targetPath: '$(System.DefaultWorkingDirectory)'

            - script: dir
              displayName: List ArmTemplates Artifact in Workspace
              workingDirectory: '$(System.DefaultWorkingDirectory)/pysparklewheel'

            - script: pip install databricks-cli
              displayName: Install Databricks CLI
              
            - script: |
                        echo "$(databricks-token)
                        $(databricks-token)" | databricks configure --token
              displayName: Configure databricks-cli

            - script: |
                        databricks workspace ls
                        databricks fs ls
              displayName: test databricks-cli

            - script: |
                      echo Destination is /FileStore
                      echo Source is $(System.DefaultWorkingDirectory)/Databricks/DBFS-Files
                      databricks fs cp -r --overwrite $(System.DefaultWorkingDirectory)/pysparklewheel/*.whl dbfs:/FileStore/pysparkl_latest.whl

              displayName: Upload whl



  
  - stage: Release
    dependsOn:
        - Build_Stage
        - Deploy_Prod_Stage
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'), eq(variables['create_release'], 'true'))
    variables:
        - group: amido-stacks-infra-credentials-nonprod
        - name: version_number
          value: "$(version_major).$(version_minor).$(version_revision)"
    jobs:
        - job: CreateGitHubRelease
          pool:
            name: $(agentpool_name)
          steps:
            # Check out the repo so that it can be tagged
            - checkout: self
              persistCredentials: true

            # Create a tag in the code for this release
            - task: Bash@3
              displayName: Tag Code
              inputs:
                targetType: "inline"
                script: |
                  git config user.name "BuildService"
                  git config user.email "builder@${COMPANY}.com"
                  git tag -a v${VERSION_NUMBER} -m "Release created by Azure DevOps"
                  git push origin v${VERSION_NUMBER}
              env:
                COMPANY: $(company)

            # Create a GitHub release with these packages
            - task: GitHubRelease@1
              displayName: Create GitHub Release
              inputs:
                gitHubConnection: $(github_release_service_connection)
                repositoryName: $(github_org)/$(self_repo)
                tag: v${VERSION_NUMBER}
                releaseNotesSource: 'inline'
                releaseNotesInline: "$(version_major).$(version_minor).$(version_revision)"
                tagSource: 'gitTag'
                changeLogCompareToRelease: 'lastFullRelease'
                changeLogType: 'commitBased'
