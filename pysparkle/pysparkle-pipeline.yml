name: "$(Build.SourceBranchName)-init"

#trigger:
#- main #collaboration branch
#- none

pr:
  branches:
    include:
    - 'main'
  paths:
    include:
      - pysparkle/*
      - pysparkle/pysparkle-pipeline.yml
trigger:
  branches:
    include:
      - 'main'
  paths:
    include:
      - pysparkle/*
      - pysparkle/pysparkle-pipeline.yml
variables:
  - template: ../de_build/job-pipeline-vars.yml
  - template: ../de_build/air-data-testing-vars.yml
  - name: self_repo_pysparkle_dir
    value: $(self_repo_dir)/pysparkle
  - name: pythonVersion
    value: '3.10.10'
  - name: test_unit_path
    value: '$(self_repo_dir)/pysparkle/tests/unit/'
  - name: junit_path
    value: '$(self_repo_dir)/pysparkle/junit/'
pool:
  name: $(agentpool_name)
stages:
  - stage: Build_Stage
    variables:
      - group: amido-stacks-de-pipeline-nonprod
      - name: version_number
        value: $(version_major).$(version_minor).$(version_revision)
    jobs:
      - job: Build_Pysparkle
        displayName: Build_Pysparkle
        steps:
          - task: UsePythonVersion@0
            inputs:
              versionSpec: $(pythonVersion)
              githubToken: $(github_token)
              addToPath: true
            displayName: Set Python Version
          - bash: |
              pip install pylint pylint-exit pytest-cov poetry
            displayName: Install Pipeline Tools
          - bash: |
              poetry install 
            displayName: Install Dependencies
            workingDirectory: $(self_repo_pysparkle_dir)
          - bash: |
              poetry run pytest -rf -v tests -k unit
            displayName: 'Run Unit Tests'
            workingDirectory:  '$(self_repo_pysparkle_dir)'          
          - bash: |
              poetry build
            displayName: Build Project
            workingDirectory: '$(self_repo_pysparkle_dir)/pysparkle'
          - task: CopyFiles@2
            displayName: Copy Whl
            inputs:
              SourceFolder: '$(self_repo_pysparkle_dir)/dist/'
              Contents: '*.whl'
              TargetFolder: '$(Build.ArtifactStagingDirectory)/pysparklewheel'
          - task: PublishBuildArtifacts@1
            displayName: 'Publish Artifact: pysparklewheel'
            inputs:
              ArtifactName: pysparklewheel
              PathtoPublish: '$(Build.ArtifactStagingDirectory)/pysparklewheel'

#############################################################
# Deploy to non Prod
#############################################################
  - stage: Deploy_NonPROD_Stage
    variables:
        - group: amido-stacks-de-pipeline-nonprod
        - group: amido-stacks-infra-credentials-nonprod
        - name: version_number
          value: "$(version_major).$(version_minor).$(version_revision)"
    dependsOn: Build_Stage
    jobs:
      - deployment: Deploy_NonPROD
        displayName: 'Deploy To NonPROD'
        environment: ${{ variables.domain }}-nonprod
        pool:
          name: $(agentpool_name)
        strategy:
          runOnce:
            deploy:  
              steps:
              - task: DownloadPipelineArtifact@2
                displayName: Download Build Artifacts
                inputs:
                  targetPath: '$(System.DefaultWorkingDirectory)'
              - script: dir
                displayName: List Artifact in Workspace
                workingDirectory: '$(System.DefaultWorkingDirectory)/pysparklewheel'
              - task: AzureKeyVault@2
                inputs:
                    azureSubscription: 'Stacks.Pipeline.Builds'
                    KeyVaultName: $(keyvault_name)
                    SecretsFilter: '*'
                    RunAsPreJob: false
                displayName: 'Get Keyvault Secrets' 
              - script: |
                          echo "$(databricks-host)
                          $(databricks-token)" | databricks configure --token
                displayName: Configure Databricks-cli
              - script: |
                        echo Destination is /FileStore/jars
                        echo Source is $(System.DefaultWorkingDirectory)/pysparklewheel/
                        databricks fs cp -r --overwrite $(System.DefaultWorkingDirectory)/pysparklewheel/*.whl dbfs:/FileStore/jars/pysparkle-latest-py3-none-any.whl
                displayName: Upload whl


  #############################################################
  # Deploy to Prod
  #############################################################
  - stage: Deploy_Prod_Stage
    dependsOn: 
      - Build_Stage
      - Deploy_NonPROD_Stage
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
    variables:
        - group: amido-stacks-de-pipeline-prod
        - name: version_number
          value: "$(version_major).$(version_minor).$(version_revision)"
    jobs:
      - deployment: Deploy_PRDO
        displayName: 'Deploy To PROD'
        environment: ${{ variables.domain }}-prod
        pool:
          name: $(agentpool_name)
        strategy:
          runOnce:
            deploy:  
              steps:
              - task: DownloadPipelineArtifact@2
                displayName: Download Build Artifacts
                inputs:
                  targetPath: '$(System.DefaultWorkingDirectory)'
              - script: dir
                displayName: List ArmTemplates Artifact in Workspace
                workingDirectory: '$(System.DefaultWorkingDirectory)/pysparklewheel'
              - task: AzureKeyVault@2
                inputs:
                    azureSubscription: 'Stacks.Pipeline.Builds'
                    KeyVaultName: $(keyvault_name)
                    SecretsFilter: '*'
                    RunAsPreJob: false
                displayName: 'Get Keyvault Secrets'            
              - script: |
                          echo "$(databricks-host)
                          $(databricks-token)" | databricks configure --token
                displayName: Configure Databricks-cli
              - script: |
                        echo Destination is /FileStore/jars
                        echo Source is $(System.DefaultWorkingDirectory)/pysparklewheel/
                        databricks fs cp -r --overwrite $(System.DefaultWorkingDirectory)/pysparklewheel/*.whl dbfs:/FileStore/jars/pysparkle-latest-py3-none-any.whl
                displayName: Upload whl

  - stage: Release
    dependsOn:
        - Build_Stage
        - Deploy_Prod_Stage
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'), eq(variables['create_release'], 'true'))
    variables:
        - group: amido-stacks-infra-credentials-nonprod
        - name: version_number
          value: "$(version_major).$(version_minor).$(version_revision)"
    jobs:
        - job: CreateGitHubRelease
          pool:
            name: $(agentpool_name)
          steps:
            # Check out the repo so that it can be tagged
            - checkout: self
              persistCredentials: true

            # Create a tag in the code for this release
            - task: Bash@3
              displayName: Tag Code
              inputs:
                targetType: "inline"
                script: |
                  git config user.name "BuildService"
                  git config user.email "builder@${COMPANY}.com"
                  git tag -a v${VERSION_NUMBER} -m "Release created by Azure DevOps"
                  git push origin v${VERSION_NUMBER}
              env:
                COMPANY: $(company)

            # Create a GitHub release with these packages
            - task: GitHubRelease@1
              displayName: Create GitHub Release
              inputs:
                gitHubConnection: $(github_release_service_connection)
                repositoryName: $(github_org)/$(self_repo)
                tag: v${VERSION_NUMBER}
                releaseNotesSource: 'inline'
                releaseNotesInline: "$(version_major).$(version_minor).$(version_revision)"
                tagSource: 'gitTag'
                changeLogCompareToRelease: 'lastFullRelease'
                changeLogType: 'commitBased'
